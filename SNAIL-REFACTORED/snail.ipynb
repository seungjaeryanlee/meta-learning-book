{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Attentive Meta-Learner (SNAIL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `blocks.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CasualConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, \n",
    "                 stride=1, dilation=1, groups=1, bias=True):\n",
    "        super(CasualConv1d, self).__init__()\n",
    "        self.dilation = dilation\n",
    "        padding = dilation * (kernel_size - 1)\n",
    "        self.conv1d = nn.Conv1d(in_channels, out_channels, kernel_size, stride,\n",
    "                                padding, dilation, groups, bias)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Takes something of shape (N, in_channels, T),\n",
    "        # returns (N, out_channels, T)\n",
    "        out = self.conv1d(input)\n",
    "        return out[:, :, :-self.dilation] # TODO: make this correct for different strides/padding\n",
    "\n",
    "    \n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, in_channels, dilation, filters, kernel_size=2):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        self.casualconv1 = CasualConv1d(in_channels, filters, kernel_size, dilation=dilation)\n",
    "        self.casualconv2 = CasualConv1d(in_channels, filters, kernel_size, dilation=dilation)\n",
    "\n",
    "    def forward(self, input_):\n",
    "        # input is dimensions (N, in_channels, T)\n",
    "        xf = self.casualconv1(input_)\n",
    "        xg = self.casualconv2(input_)\n",
    "        activations = torch.tanh(xf) * torch.sigmoid(xg) # shape: (N, filters, T)\n",
    "\n",
    "        return torch.cat((input_, activations), dim=1)\n",
    "\n",
    "class TCBlock(nn.Module):\n",
    "    def __init__(self, in_channels, seq_length, filters):\n",
    "        super(TCBlock, self).__init__()\n",
    "        self.dense_blocks = nn.ModuleList([DenseBlock(in_channels + i * filters, 2 ** (i+1), filters)\n",
    "                                           for i in range(int(math.ceil(math.log(seq_length, 2))))])\n",
    "\n",
    "    def forward(self, input):\n",
    "        # input is dimensions (N, T, in_channels)\n",
    "        input = torch.transpose(input, 1, 2)\n",
    "        for block in self.dense_blocks:\n",
    "            input = block(input)\n",
    "        return torch.transpose(input, 1, 2)\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, key_size, value_size):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.linear_query = nn.Linear(in_channels, key_size)\n",
    "        self.linear_keys = nn.Linear(in_channels, key_size)\n",
    "        self.linear_values = nn.Linear(in_channels, value_size)\n",
    "        self.sqrt_key_size = math.sqrt(key_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # input is dim (N, T, in_channels) where N is the batch_size, and T is\n",
    "        # the sequence length\n",
    "        mask = np.array([[1 if i>j else 0 for i in range(input.shape[1])] for j in range(input.shape[1])])\n",
    "        mask = torch.BoolTensor(mask).cuda()\n",
    "\n",
    "        #import pdb; pdb.set_trace()\n",
    "        keys = self.linear_keys(input) # shape: (N, T, key_size)\n",
    "        query = self.linear_query(input) # shape: (N, T, key_size)\n",
    "        values = self.linear_values(input) # shape: (N, T, value_size)\n",
    "        temp = torch.bmm(query, torch.transpose(keys, 1, 2)) # shape: (N, T, T)\n",
    "        temp.data.masked_fill_(mask, -float('inf'))\n",
    "        temp = F.softmax(temp / self.sqrt_key_size, dim=1) # shape: (N, T, T), broadcasting over any slice [:, x, :], each row of the matrix\n",
    "        temp = torch.bmm(temp, values) # shape: (N, T, value_size)\n",
    "        return torch.cat((input, temp), dim=2) # shape: (N, T, in_channels + value_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `resnet_blocks.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(in_planes, out_planes, kernel_size=3, stride=1, padding=1):\n",
    "    \"\"\"convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n",
    "                     padding=padding, bias=False)\n",
    "\n",
    "def conv_block(in_channels, out_channels):\n",
    "    '''\n",
    "    returns a block conv-bn-relu-pool\n",
    "    '''\n",
    "    return nn.Sequential(OrderedDict([\n",
    "        ('conv', nn.Conv2d(in_channels, out_channels, 3, padding=1)),\n",
    "        ('bn', nn.BatchNorm2d(out_channels, momentum=1)),\n",
    "        #('bn', nn.BatchNorm2d(out_channels)),\n",
    "        ('relu', nn.ReLU()),\n",
    "        ('pool', nn.MaxPool2d(2))\n",
    "    ]))\n",
    "\n",
    "def batchnorm(input, weight=None, bias=None, running_mean=None, running_var=None, training=True,eps=1e-5, momentum=0.1):\n",
    "    # momentum = 1 restricts stats to the current mini-batch\n",
    "    # This hack only works when momentum is 1 and avoids needing to track\n",
    "    # running stats by substituting dummy variables\n",
    "    size = int(np.prod(np.array(input.data.size()[1])))\n",
    "    running_mean = torch.zeros(size).cuda()\n",
    "    running_var = torch.ones(size).cuda()\n",
    "    return F.batch_norm(input, running_mean, running_var, weight, bias, training, momentum, eps)\n",
    "\n",
    "class OmniglotNet(nn.Module):\n",
    "    '''\n",
    "    Model as described in the reference paper,\n",
    "    source: https://github.com/jakesnell/prototypical-networks/blob/f0c48808e496989d01db59f86d4449d7aee9ab0c/protonets/models/few_shot.py#L62-L84\n",
    "    '''\n",
    "    def __init__(self, x_dim=1, hid_dim=64, z_dim=64):\n",
    "        super(OmniglotNet, self).__init__()\n",
    "        self.encoder = nn.Sequential(OrderedDict([\n",
    "            ('block1', conv_block(x_dim, hid_dim)),\n",
    "            ('block2', conv_block(hid_dim, hid_dim)),\n",
    "            ('block3', conv_block(hid_dim, hid_dim)),\n",
    "            ('block4', conv_block(hid_dim, z_dim)),\n",
    "        ]))\n",
    "\n",
    "    def forward(self, x, weights=None):\n",
    "        if weights is None:\n",
    "            x = self.encoder(x)\n",
    "        else:\n",
    "            x = F.conv2d(x, weights['encoder.block1.conv.weight'], weights['encoder.block1.conv.bias'])\n",
    "            x = batchnorm(x, weight=weights['encoder.block1.bn.weight'], bias=weights['encoder.block1.bn.bias'])\n",
    "            x = F.relu(x)\n",
    "            x = F.max_pool2d(x, 2, 2)\n",
    "            x = F.conv2d(x, weights['encoder.block2.conv.weight'], weights['encoder.block2.conv.bias'])\n",
    "            x = batchnorm(x, weight=weights['encoder.block2.bn.weight'], bias=weights['encoder.block2.bn.bias'])\n",
    "            x = F.relu(x)\n",
    "            x = F.max_pool2d(x, 2, 2)\n",
    "            x = F.conv2d(x, weights['encoder.block3.conv.weight'], weights['encoder.block3.conv.bias'])\n",
    "            x = batchnorm(x, weight=weights['encoder.block3.bn.weight'], bias=weights['encoder.block3.bn.bias'])\n",
    "            x = F.relu(x)\n",
    "            x = F.max_pool2d(x, 2, 2)\n",
    "            x = F.conv2d(x, weights['encoder.block4.conv.weight'], weights['encoder.block4.conv.bias'])\n",
    "            x = batchnorm(x, weight=weights['encoder.block4.bn.weight'], bias=weights['encoder.block4.bn.bias'])\n",
    "            x = F.relu(x)\n",
    "            x = F.max_pool2d(x, 2, 2)\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, filters, pool_padding=0):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = conv(in_channels, filters)\n",
    "        self.bn1 = nn.BatchNorm2d(filters)\n",
    "        self.relu1 = nn.LeakyReLU()\n",
    "        self.conv2 = conv(filters, filters)\n",
    "        self.bn2 = nn.BatchNorm2d(filters)\n",
    "        self.relu2 = nn.LeakyReLU()\n",
    "        self.conv3 = conv(filters, filters)\n",
    "        self.bn3 = nn.BatchNorm2d(filters)\n",
    "        self.relu3 = nn.LeakyReLU()\n",
    "        self.conv4 = conv(in_channels, filters, kernel_size=1, padding=0)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(2, padding=pool_padding)\n",
    "        self.dropout = nn.Dropout(p=0.9)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.conv4(x)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu2(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.relu3(out)\n",
    "\n",
    "        out += residual\n",
    "        out = self.maxpool(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `snail.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnailFewShot(nn.Module):\n",
    "    def __init__(self, N, K, task, use_cuda=True):\n",
    "        # N-way, K-shot\n",
    "        super(SnailFewShot, self).__init__()\n",
    "        if task == 'omniglot':\n",
    "            self.encoder = OmniglotNet()\n",
    "            num_channels = 64 + N\n",
    "        elif task == 'mini_imagenet':\n",
    "            self.encoder = MiniImagenetNet()\n",
    "            num_channels = 384 + N\n",
    "        else:\n",
    "            raise ValueError('Not recognized task value')\n",
    "        num_filters = int(math.ceil(math.log(N * K + 1, 2)))\n",
    "        self.attention1 = AttentionBlock(num_channels, 64, 32)\n",
    "        num_channels += 32\n",
    "        self.tc1 = TCBlock(num_channels, N * K + 1, 128)\n",
    "        num_channels += num_filters * 128\n",
    "        self.attention2 = AttentionBlock(num_channels, 256, 128)\n",
    "        num_channels += 128\n",
    "        self.tc2 = TCBlock(num_channels, N * K + 1, 128)\n",
    "        num_channels += num_filters * 128\n",
    "        self.attention3 = AttentionBlock(num_channels, 512, 256)\n",
    "        num_channels += 256\n",
    "        self.fc = nn.Linear(num_channels, N)\n",
    "        self.N = N\n",
    "        self.K = K\n",
    "        self.use_cuda = use_cuda\n",
    "\n",
    "    def forward(self, input, labels):\n",
    "        x = self.encoder(input)\n",
    "        batch_size = int(labels.size()[0] / (self.N * self.K + 1))\n",
    "\n",
    "        # TODO: Maybe move this zeroing to process_torchmeta_batch() at train.py\n",
    "        last_idxs = [(i + 1) * (self.N * self.K + 1) - 1 for i in range(batch_size)]\n",
    "        if self.use_cuda:\n",
    "            labels[last_idxs] = torch.Tensor(np.zeros((batch_size, labels.size()[1]))).cuda()\n",
    "        else:\n",
    "            labels[last_idxs] = torch.Tensor(np.zeros((batch_size, labels.size()[1])))\n",
    "\n",
    "        x = torch.cat((x, labels), 1)\n",
    "        x = x.view((batch_size, self.N * self.K + 1, -1))\n",
    "        x = self.attention1(x)\n",
    "        x = self.tc1(x)\n",
    "        x = self.attention2(x)\n",
    "        x = self.tc2(x)\n",
    "        x = self.attention3(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `train.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "\"\"\"\n",
    "Main script for training SNAIL on Omniglot.\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from networks import SnailFewShot\n",
    "\n",
    "from torchmeta.datasets.helpers import omniglot\n",
    "from torchmeta.utils.data import BatchMetaDataLoader\n",
    "\n",
    "\n",
    "def get_acc(last_model, last_targets):\n",
    "    _, preds = last_model.max(1)\n",
    "    acc = torch.eq(preds, last_targets).float().mean()\n",
    "    return acc.item()\n",
    "\n",
    "\n",
    "def process_torchmeta_batch(batch, options):\n",
    "    \"\"\"\n",
    "    Process batch from torchmeta dataset for the SNAIL model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch : dict\n",
    "        A dictionary given by the torchmeta dataset\n",
    "    options : SimpleNamespace\n",
    "        A namespace with configuration details.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    input_images : torch.tensor\n",
    "        Input images to the SNAIL model of shape (batch_size*(N*K+1), img_channels, img_height,\n",
    "        img_width) where N and K denote the same variables as in the N-way K-shot problem.\n",
    "    input_onehot_labels : torch.tensor\n",
    "        Input one-hot labels to the SNAIL model of shape (batch_size*(N*K+1), N) where N and K\n",
    "        denote the same variables as in the N-way K-shot problem. The last label for each (N*K+1)\n",
    "        is not used since it is the target label.\n",
    "    target_labels : torch.tensor\n",
    "        Test set labels to evaluate the SNAIL model by comparing with its outputs. Has shape\n",
    "        (batch_size).\n",
    "\n",
    "    \"\"\"\n",
    "    train_inputs, train_labels = batch[\"train\"]\n",
    "    test_inputs, test_labels = batch[\"test\"]\n",
    "\n",
    "    # Select one image from N images in the test set\n",
    "    chosen_indices = torch.randint(test_inputs.shape[1], size=(test_inputs.shape[0],))\n",
    "    chosen_test_inputs = test_inputs[torch.arange(test_inputs.shape[0]), chosen_indices, :, :, :].unsqueeze(1)\n",
    "    chosen_test_labels = test_labels[torch.arange(test_labels.shape[0]), chosen_indices].unsqueeze(1)\n",
    "\n",
    "    # Concatenate train and test set for SNAIL-style input images and labels\n",
    "    input_images = torch.cat((train_inputs, chosen_test_inputs), dim=1).reshape((-1, *train_inputs.shape[2:]))\n",
    "    input_labels = torch.cat((train_labels, chosen_test_labels), dim=1).reshape((-1, *train_labels.shape[2:]))\n",
    "\n",
    "    # Convert labels to one-hot\n",
    "    input_onehot_labels = F.one_hot(input_labels).float()\n",
    "\n",
    "    # Separate out target labels\n",
    "    target_labels = input_labels[::(options.num_cls * options.num_samples + 1)].long()\n",
    "\n",
    "    # Move to correct device\n",
    "    if options.cuda:\n",
    "        input_images, input_onehot_labels = input_images.cuda(), input_onehot_labels.cuda()\n",
    "        target_labels = target_labels.cuda()\n",
    "\n",
    "    return input_images, input_onehot_labels, target_labels\n",
    "\n",
    "\n",
    "def train(model, optimizer, train_dataloader, val_dataloader, opt):\n",
    "    if val_dataloader is None:\n",
    "        best_state = None\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "    best_acc = 0\n",
    "\n",
    "    best_model_path = os.path.join(opt.exp, 'best_model.pth')\n",
    "    last_model_path = os.path.join(opt.exp, 'last_model.pth')\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(opt.epochs):\n",
    "        print('=== Epoch: {} ==='.format(epoch))\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        for i, batch in tqdm(enumerate(train_dataloader), total=1000):\n",
    "            if i >= 1000:\n",
    "                break\n",
    "            input_images, input_onehot_labels, target_labels = process_torchmeta_batch(batch, opt)\n",
    "            predicted_labels = model(input_images, input_onehot_labels)[:, -1, :]\n",
    "            loss = loss_fn(predicted_labels, target_labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "            train_acc.append(get_acc(predicted_labels, target_labels))\n",
    "\n",
    "        avg_loss = np.mean(train_loss[-opt.iterations:])\n",
    "        avg_acc = np.mean(train_acc[-opt.iterations:])\n",
    "        print('Avg Train Loss: {}, Avg Train Acc: {}'.format(avg_loss, avg_acc))\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        for i, batch in tqdm(enumerate(val_dataloader), total=1000):\n",
    "            if i >= 1000:\n",
    "                break\n",
    "            input_images, input_onehot_labels, target_labels = process_torchmeta_batch(batch, opt)\n",
    "            predicted_labels = model(input_images, input_onehot_labels)[:, -1, :]\n",
    "            loss = loss_fn(predicted_labels, target_labels)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "            val_acc.append(get_acc(predicted_labels, target_labels))\n",
    "\n",
    "        avg_loss = np.mean(val_loss[-opt.iterations:])\n",
    "        avg_acc = np.mean(val_acc[-opt.iterations:])\n",
    "\n",
    "        postfix = ' (Best)' if avg_acc >= best_acc else ' (Best: {})'.format(best_acc)\n",
    "        print('Avg Val Loss: {}, Avg Val Acc: {}{}'.format(avg_loss, avg_acc, postfix))\n",
    "        if avg_acc >= best_acc:\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            best_acc = avg_acc\n",
    "            best_state = model.state_dict()\n",
    "\n",
    "        # TODO(seungjaeryanlee): Understand this code better\n",
    "        for name in ['train_loss', 'train_acc', 'val_loss', 'val_acc']:\n",
    "            with open(os.path.join(opt.exp, name + '.txt'), 'w') as f:\n",
    "                for item in locals()[name]:\n",
    "                    f.write(\"%s\\n\" % item)\n",
    "\n",
    "    torch.save(model.state_dict(), last_model_path)\n",
    "\n",
    "    return best_state, best_acc, train_loss, train_acc, val_loss, val_acc\n",
    "\n",
    "\n",
    "def test(model, test_dataloader, opt):\n",
    "    \"\"\"\n",
    "    Test model on given dataset and options.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    acc_per_epoch = []\n",
    "    for epoch in range(opt.test_epochs):\n",
    "        for i, batch in tqdm(enumerate(test_dataloader), total=1000):\n",
    "            if i >= 1000:\n",
    "                break\n",
    "            input_images, input_onehot_labels, target_labels = process_torchmeta_batch(batch, opt)\n",
    "            predicted_labels = model(input_images, input_onehot_labels)[:, -1, :]\n",
    "\n",
    "            acc_per_epoch.append(get_acc(predicted_labels, target_labels))\n",
    "\n",
    "    avg_acc = np.mean(acc_per_epoch)\n",
    "    print('Test Acc: {}'.format(avg_acc))\n",
    "\n",
    "    return avg_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Epoch: 0 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:45<00:00, 21.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Train Loss: 0.5472219952940941, Avg Train Acc: 0.765125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:42<00:00, 23.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Val Loss: 0.0045445125997066495, Avg Val Acc: 1.0 (Best)\n",
      "Testing with last model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:42<00:00, 23.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc: 1.0\n",
      "Testing with best model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:42<00:00, 23.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--exp', type=str, default='default')\n",
    "# parser.add_argument('--epochs', type=int, default=100)\n",
    "# parser.add_argument('--test-epochs', type=int, default=1)\n",
    "# parser.add_argument('--iterations', type=int, default=10000)\n",
    "# parser.add_argument('--dataset', type=str, default='omniglot')\n",
    "# parser.add_argument('--num_cls', type=int, default=5)\n",
    "# parser.add_argument('--num_samples', type=int, default=1)\n",
    "# parser.add_argument('--lr', type=float, default=0.0001)\n",
    "# parser.add_argument('--batch_size', type=int, default=32)\n",
    "# parser.add_argument('--cuda', action='store_true')\n",
    "# options = parser.parse_args()\n",
    "\n",
    "from types import SimpleNamespace\n",
    "options = SimpleNamespace(**{\n",
    "    \"exp\": \"default\",\n",
    "    \"epochs\": 1,\n",
    "    \"test_epochs\": 1,\n",
    "    \"iterations\": 10000,\n",
    "    \"dataset\": \"omniglot\",\n",
    "    \"num_cls\": 5,\n",
    "    \"num_samples\": 1,\n",
    "    \"lr\": 0.0001,\n",
    "    \"batch_size\": 32,\n",
    "    \"cuda\": True,\n",
    "})\n",
    "\n",
    "if not os.path.exists(options.exp):\n",
    "    os.makedirs(options.exp)\n",
    "\n",
    "if torch.cuda.is_available() and not options.cuda:\n",
    "    print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "\n",
    "# Setup dataset\n",
    "train_dataset = omniglot(\"data\", ways=5, shots=1, test_shots=1, meta_train=True, download=True)\n",
    "train_dataloader = BatchMetaDataLoader(train_dataset, batch_size=options.batch_size, num_workers=8)\n",
    "val_dataset = omniglot(\"data\", ways=5, shots=1, test_shots=1, meta_val=True, download=True)\n",
    "val_dataloader = BatchMetaDataLoader(val_dataset, batch_size=options.batch_size, num_workers=8)\n",
    "test_dataset = omniglot(\"data\", ways=5, shots=1, test_shots=1, meta_test=True, download=True)\n",
    "test_dataloader = BatchMetaDataLoader(test_dataset, batch_size=options.batch_size, num_workers=8)\n",
    "# Setup model\n",
    "model = SnailFewShot(options.num_cls, options.num_samples, options.dataset, options.cuda)\n",
    "model = model.cuda() if options.cuda else model\n",
    "# Setup optimizer\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=options.lr)\n",
    "\n",
    "# Train model\n",
    "train_result = train(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    opt=options,\n",
    ")\n",
    "best_state, best_acc, train_loss, train_acc, val_loss, val_acc = train_result\n",
    "\n",
    "# Test last model\n",
    "print('Testing with last model..')\n",
    "test(\n",
    "    model=model,\n",
    "    test_dataloader=test_dataloader,\n",
    "    opt=options,\n",
    ")\n",
    "\n",
    "# Test best model\n",
    "model.load_state_dict(best_state)\n",
    "print('Testing with best model..')\n",
    "test(\n",
    "    opt=options,\n",
    "    test_dataloader=test_dataloader,\n",
    "    model=model,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
