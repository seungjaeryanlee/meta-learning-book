{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Attentive Meta-Learner (SNAIL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Allow GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Fix multiprocessing warning from num_workers in torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore InterpolationMode warnings from torchmeta\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import math\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchmeta.datasets.helpers import omniglot\n",
    "from torchmeta.utils.data import BatchMetaDataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### Embedding Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/omniglot_embedding.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self, x_dim=1, hid_dim=64, z_dim=64):\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            EmbeddingNet._conv_block(x_dim, hid_dim),\n",
    "            EmbeddingNet._conv_block(hid_dim, hid_dim),\n",
    "            EmbeddingNet._conv_block(hid_dim, hid_dim),\n",
    "            EmbeddingNet._conv_block(hid_dim, z_dim),\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _conv_block(in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels, momentum=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        out = x.view(x.size(0), -1)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### SNAIL Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "#### Causal Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use this instead: https://github.com/pytorch/pytorch/issues/1333#issuecomment-400338207 ?\n",
    "class CasualConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, groups=1, bias=True):\n",
    "        super(CasualConv1d, self).__init__()\n",
    "        self.dilation = dilation\n",
    "        padding = dilation * (kernel_size - 1)\n",
    "        self.conv1d = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n",
    "\n",
    "    def forward(self, input_):\n",
    "        # Takes something of shape (N, in_channels, T), returns (N, out_channels, T)\n",
    "        out = self.conv1d(input_)\n",
    "        return out[:, :, :-self.dilation] # TODO: make this correct for different strides/padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "#### Dense Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/snail_dense_block.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, in_channels, dilation, filters, kernel_size=2):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        self.causal_conv1 = CasualConv1d(in_channels, filters, kernel_size, dilation=dilation)\n",
    "        self.causal_conv2 = CasualConv1d(in_channels, filters, kernel_size, dilation=dilation)\n",
    "\n",
    "    def forward(self, input_):\n",
    "        # input is dimensions (N, in_channels, T)\n",
    "        xf = self.causal_conv1(input_)\n",
    "        xg = self.causal_conv2(input_)\n",
    "        activations = torch.tanh(xf) * torch.sigmoid(xg) # shape: (N, filters, T)\n",
    "\n",
    "        return torch.cat((input_, activations), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "#### TC Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/snail_tc_block.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCBlock(nn.Module):\n",
    "    def __init__(self, in_channels, seq_length, filters):\n",
    "        super(TCBlock, self).__init__()\n",
    "        self.dense_blocks = nn.ModuleList([DenseBlock(in_channels + i * filters, 2 ** (i+1), filters)\n",
    "                                           for i in range(int(math.ceil(math.log(seq_length, 2))))])\n",
    "\n",
    "    def forward(self, input_):\n",
    "        # input is dimensions (N, T, in_channels)\n",
    "        input_ = torch.transpose(input_, 1, 2)\n",
    "        for block in self.dense_blocks:\n",
    "            input_ = block(input_)\n",
    "\n",
    "        return torch.transpose(input_, 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "#### Attention Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/snail_attention_block.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, key_size, value_size):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.query_layer = nn.Linear(in_channels, key_size)\n",
    "        self.keys_layer = nn.Linear(in_channels, key_size)\n",
    "        self.values_layer = nn.Linear(in_channels, value_size)\n",
    "        self.sqrt_key_size = math.sqrt(key_size)\n",
    "        self.key_size = key_size\n",
    "\n",
    "    @staticmethod\n",
    "    def CausallyMaskedSoftmax(logits, key_size):\n",
    "        seq_len = logits.shape[1]\n",
    "        mask = np.array([[i > j for i in range(seq_len)] for j in range(seq_len)])\n",
    "        mask = torch.BoolTensor(mask)\n",
    "        # mask = torch.BoolTensor(mask).cuda()\n",
    "        logits = logits.data.masked_fill(mask, -float('inf'))\n",
    "\n",
    "        return F.softmax(logits / math.sqrt(key_size), dim=1)\n",
    "\n",
    "    def forward(self, input_):\n",
    "        # input is dim (N, T, in_channels) where N is the batch_size, and T is the sequence length\n",
    "        keys = self.keys_layer(input_) # shape: (N, T, key_size)\n",
    "        query = self.query_layer(input_) # shape: (N, T, key_size)\n",
    "        logits = torch.bmm(query, torch.transpose(keys, 1, 2)) # shape: (N, T, T)\n",
    "        probs = AttentionBlock.CausallyMaskedSoftmax(logits, self.key_size) # shape: (N, T, T), broadcasting over any slice [:, x, :], each row of the matrix\n",
    "\n",
    "        values = self.values_layer(input_) # shape: (N, T, value_size)\n",
    "        read = torch.bmm(probs, values) # shape: (N, T, value_size)\n",
    "\n",
    "        return torch.cat((input_, read), dim=2) # shape: (N, T, in_channels + value_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### Full Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/snail_architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNAIL(nn.Module):\n",
    "    def __init__(self, N, K):\n",
    "        super(SNAIL, self).__init__()\n",
    "\n",
    "        self.N, self.K = N, K # N-way, K-shot\n",
    "\n",
    "        self.encoder = EmbeddingNet()\n",
    "        num_channels = 64 + N\n",
    "        num_filters = int(math.ceil(math.log(N * K + 1, 2)))\n",
    "        self.attention1 = AttentionBlock(num_channels, 64, 32)\n",
    "        num_channels += 32\n",
    "        self.tc1 = TCBlock(num_channels, N * K + 1, 128)\n",
    "        num_channels += num_filters * 128\n",
    "        self.attention2 = AttentionBlock(num_channels, 256, 128)\n",
    "        num_channels += 128\n",
    "        self.tc2 = TCBlock(num_channels, N * K + 1, 128)\n",
    "        num_channels += num_filters * 128\n",
    "        self.attention3 = AttentionBlock(num_channels, 512, 256)\n",
    "        num_channels += 256\n",
    "        self.fc = nn.Linear(num_channels, N)\n",
    "\n",
    "    def forward(self, input, labels):\n",
    "        x = self.encoder(input)\n",
    "        batch_size = int(labels.size()[0] / (self.N * self.K + 1))\n",
    "\n",
    "        # TODO: Maybe move this zeroing to process_torchmeta_batch() at train.py\n",
    "        last_idxs = [(i + 1) * (self.N * self.K + 1) - 1 for i in range(batch_size)]\n",
    "#         labels[last_idxs] = torch.Tensor(np.zeros((batch_size, labels.size()[1]))).cuda()\n",
    "        labels[last_idxs] = torch.Tensor(np.zeros((batch_size, labels.size()[1])))\n",
    "\n",
    "        x = torch.cat((x, labels), 1)\n",
    "        x = x.view((batch_size, self.N * self.K + 1, -1))\n",
    "        x = self.attention1(x)\n",
    "        x = self.tc1(x)\n",
    "        x = self.attention2(x)\n",
    "        x = self.tc2(x)\n",
    "        x = self.attention3(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Improve parameter names\n",
    "def get_acc(last_model, last_targets):\n",
    "    _, preds = last_model.max(1)\n",
    "    acc = torch.eq(preds, last_targets).float().mean()\n",
    "    return acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Update docstring\n",
    "def process_torchmeta_batch(batch, NUM_CLS, NUM_SAMPLES):\n",
    "    \"\"\"\n",
    "    Process batch from torchmeta dataset for the SNAIL model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch : dict\n",
    "        A dictionary given by the torchmeta dataset\n",
    "    options : SimpleNamespace\n",
    "        A namespace with configuration details.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    input_images : torch.tensor\n",
    "        Input images to the SNAIL model of shape (batch_size*(N*K+1), img_channels, img_height,\n",
    "        img_width) where N and K denote the same variables as in the N-way K-shot problem.\n",
    "    input_onehot_labels : torch.tensor\n",
    "        Input one-hot labels to the SNAIL model of shape (batch_size*(N*K+1), N) where N and K\n",
    "        denote the same variables as in the N-way K-shot problem. The last label for each (N*K+1)\n",
    "        is not used since it is the target label.\n",
    "    target_labels : torch.tensor\n",
    "        Test set labels to evaluate the SNAIL model by comparing with its outputs. Has shape\n",
    "        (batch_size).\n",
    "\n",
    "    \"\"\"\n",
    "    train_inputs, train_labels = batch[\"train\"]\n",
    "    test_inputs, test_labels = batch[\"test\"]\n",
    "\n",
    "    # Select one image from N images in the test set\n",
    "    chosen_indices = torch.randint(test_inputs.shape[1], size=(test_inputs.shape[0],))\n",
    "    chosen_test_inputs = test_inputs[torch.arange(test_inputs.shape[0]), chosen_indices, :, :, :].unsqueeze(1)\n",
    "    chosen_test_labels = test_labels[torch.arange(test_labels.shape[0]), chosen_indices].unsqueeze(1)\n",
    "\n",
    "    # Concatenate train and test set for SNAIL-style input images and labels\n",
    "    input_images = torch.cat((train_inputs, chosen_test_inputs), dim=1).reshape((-1, *train_inputs.shape[2:]))\n",
    "    input_labels = torch.cat((train_labels, chosen_test_labels), dim=1).reshape((-1, *train_labels.shape[2:]))\n",
    "\n",
    "    # Convert labels to one-hot\n",
    "    input_onehot_labels = F.one_hot(input_labels).float()\n",
    "\n",
    "    # Separate out target labels\n",
    "    target_labels = input_labels[::(NUM_CLS * NUM_SAMPLES + 1)].long()\n",
    "\n",
    "    # Move to correct device\n",
    "#     if options.cuda:\n",
    "#         input_images, input_onehot_labels = input_images.cuda(), input_onehot_labels.cuda()\n",
    "#         target_labels = target_labels.cuda()\n",
    "\n",
    "    return input_images, input_onehot_labels, target_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLS = 5     # N-way\n",
    "NUM_SAMPLES = 1 # K-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRAIN_EPISODES = 200\n",
    "N_VAL_EPISODES = 100\n",
    "N_TEST_EPISODES = 100\n",
    "\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "VAL_BATCH_SIZE = 32\n",
    "TEST_BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADAM_LR = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"default\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA =  torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main script for training SNAIL on Omniglot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_episode(model, optimizer, batch, NUM_CLS, NUM_SAMPLES):\n",
    "    input_images, input_onehot_labels, target_labels = process_torchmeta_batch(batch, NUM_CLS, NUM_SAMPLES)\n",
    "    predicted_labels = model(input_images, input_onehot_labels)[:, -1, :]\n",
    "    loss = F.cross_entropy(predicted_labels, target_labels)\n",
    "    acc = get_acc(predicted_labels, target_labels)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item(), acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_one_episode(model, optimizer, batch, NUM_CLS, NUM_SAMPLES):\n",
    "    input_images, input_onehot_labels, target_labels = process_torchmeta_batch(batch, NUM_CLS, NUM_SAMPLES)\n",
    "    predicted_labels = model(input_images, input_onehot_labels)[:, -1, :]\n",
    "    loss = F.cross_entropy(predicted_labels, target_labels)\n",
    "    acc = get_acc(predicted_labels, target_labels)\n",
    "\n",
    "    return loss.item(), acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_dataloader, val_dataloader):\n",
    "    if val_dataloader is None:\n",
    "        best_state = None\n",
    "    train_loss_per_ep, train_acc_per_ep = [], []\n",
    "    val_loss_per_ep, val_acc_per_ep = [], []\n",
    "    best_val_acc = 0\n",
    "\n",
    "    best_model_path = os.path.join(OUTPUT_DIR, 'best_model.pth')\n",
    "    last_model_path = os.path.join(OUTPUT_DIR, 'last_model.pth')\n",
    "\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    for i, batch in tqdm(enumerate(train_dataloader), total=N_TRAIN_EPISODES):\n",
    "        if i >= N_TRAIN_EPISODES: break\n",
    "\n",
    "        train_loss, train_acc = train_one_episode(model, optimizer, batch, NUM_CLS, NUM_SAMPLES)\n",
    "        train_loss_per_ep.append(train_loss)\n",
    "        train_acc_per_ep.append(train_acc)\n",
    "\n",
    "    # TODO: Don't average all episodes here\n",
    "    avg_train_loss = np.mean(train_loss_per_ep)\n",
    "    avg_train_acc = np.mean(train_acc_per_ep)\n",
    "    print('Avg Train Loss: {}, Avg Train Acc: {}'.format(avg_train_loss, avg_train_acc))\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    for i, batch in tqdm(enumerate(val_dataloader), total=N_VAL_EPISODES):\n",
    "        if i >= N_VAL_EPISODES: break\n",
    "\n",
    "        val_loss, val_acc = test_one_episode(model, optimizer, batch, NUM_CLS, NUM_SAMPLES)\n",
    "        val_loss_per_ep.append(val_loss)\n",
    "        val_acc_per_ep.append(val_acc)\n",
    "\n",
    "    avg_val_loss = np.mean(val_loss_per_ep)\n",
    "    avg_val_acc = np.mean(val_acc_per_ep)\n",
    "    postfix = ' (Best)' if avg_val_acc >= best_val_acc else ' (Best: {})'.format(best_val_acc)\n",
    "    print('Avg Val Loss: {}, Avg Val Acc: {}{}'.format(avg_val_loss, avg_val_acc, postfix))\n",
    "\n",
    "    if avg_val_acc >= best_val_acc:\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        best_val_acc = avg_val_acc\n",
    "        best_state = model.state_dict()\n",
    "\n",
    "    for name in ['train_loss_per_ep', 'train_acc_per_ep', 'val_loss_per_ep', 'val_acc_per_ep']:\n",
    "        with open(os.path.join(OUTPUT_DIR, name + '.txt'), 'w') as f:\n",
    "            for item in locals()[name]:\n",
    "                f.write(\"%s\\n\" % item)\n",
    "\n",
    "    torch.save(model.state_dict(), last_model_path)\n",
    "\n",
    "    return best_state, best_val_acc, train_loss_per_ep, train_acc_per_ep, val_loss_per_ep, val_acc_per_ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_dataloader):\n",
    "    \"\"\"\n",
    "    Test model on given dataset and options.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_loss_per_ep, test_acc_per_ep = [], []\n",
    "    for i, batch in tqdm(enumerate(test_dataloader), total=N_TEST_EPISODES):\n",
    "        if i >= N_TEST_EPISODES: break\n",
    "\n",
    "        test_loss, test_acc = test_one_episode(model, optimizer, batch, NUM_CLS, NUM_SAMPLES)\n",
    "        test_loss_per_ep.append(test_loss)\n",
    "        test_acc_per_ep.append(test_acc)\n",
    "\n",
    "    avg_test_loss = np.mean(test_loss_per_ep)\n",
    "    avg_test_acc = np.mean(test_acc_per_ep)\n",
    "    print('Avg Test Loss: {} Avg Test Acc: {}'.format(avg_test_loss, avg_test_acc))\n",
    "\n",
    "    return avg_test_loss, avg_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup dataset\n",
    "# TODO: Data is augmented like Santoro et al. (2016)\n",
    "train_dataset = omniglot(\"data\", ways=5, shots=1, test_shots=1, meta_train=True, download=True)\n",
    "train_dataloader = BatchMetaDataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, num_workers=2)\n",
    "val_dataset = omniglot(\"data\", ways=5, shots=1, test_shots=1, meta_val=True, download=True)\n",
    "val_dataloader = BatchMetaDataLoader(val_dataset, batch_size=VAL_BATCH_SIZE, num_workers=2)\n",
    "test_dataset = omniglot(\"data\", ways=5, shots=1, test_shots=1, meta_test=True, download=True)\n",
    "test_dataloader = BatchMetaDataLoader(test_dataset, batch_size=TEST_BATCH_SIZE, num_workers=2)\n",
    "# Setup model\n",
    "model = SNAIL(NUM_CLS, NUM_SAMPLES)\n",
    "# model = model.cuda() if options.cuda else model\n",
    "# Setup optimizer\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=ADAM_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 200/200 [01:54<00:00,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Train Loss: 1.6087249863147735, Avg Train Acc: 0.22625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                      | 0/100 [00:00<?, ?it/s]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f32806be5e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1324, in __del__\n",
      "Exception ignored in:     <function _MultiProcessingDataLoaderIter.__del__ at 0x7f32806be5e0>self._shutdown_workers()\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1316, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1324, in __del__\n",
      "\n",
      "    AssertionErrorself._shutdown_workers(): \n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1316, in _shutdown_workers\n",
      "    can only test a child process\n",
      "if w.is_alive():\n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "100%|████████████████████████████████████████████| 100/100 [00:25<00:00,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Val Loss: 1.5852923119068145, Avg Val Acc: 0.2296875 (Best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "train_result = train(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    ")\n",
    "best_state, best_val_acc, train_loss_per_ep, train_acc_per_ep, val_loss_per_ep, val_acc_per_ep = train_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                      | 0/100 [00:00<?, ?it/s]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f32806be5e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1324, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1316, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: Exception ignored in: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f32806be5e0>\n",
      "Traceback (most recent call last):\n",
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x7f32806be5e0>  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1324, in __del__\n",
      "\n",
      "    Traceback (most recent call last):\n",
      "self._shutdown_workers()  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1324, in __del__\n",
      "\n",
      "      File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1316, in _shutdown_workers\n",
      "self._shutdown_workers()\n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1316, in _shutdown_workers\n",
      "        if w.is_alive():\n",
      "if w.is_alive():\n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "\n",
      "AssertionError    : assert self._parent_pid == os.getpid(), 'can only test a child process'can only test a child process\n",
      "\n",
      "Exception ignored in: AssertionError<function _MultiProcessingDataLoaderIter.__del__ at 0x7f32806be5e0>: can only test a child process\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1324, in __del__\n",
      "\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1316, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "100%|████████████████████████████████████████████| 100/100 [00:26<00:00,  3.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Test Loss: 1.583022367954254 Avg Test Acc: 0.2434375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test last model\n",
    "_ = test(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f32806be5e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1324, in __del__\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f32806be5e0>\n",
      "Traceback (most recent call last):\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1316, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1324, in __del__\n",
      "        assert self._parent_pid == os.getpid(), 'can only test a child process'self._shutdown_workers()\n",
      "AssertionError: can only test a child process\n",
      "\n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1316, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f32806be5e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1324, in __del__\n",
      "    Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f32806be5e0>self._shutdown_workers()\n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1316, in _shutdown_workers\n",
      "\n",
      "Traceback (most recent call last):\n",
      "      File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1324, in __del__\n",
      "if w.is_alive():\n",
      "      File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "self._shutdown_workers()    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1316, in _shutdown_workers\n",
      "\n",
      "AssertionError: can only test a child process    \n",
      "if w.is_alive():\n",
      "Exception ignored in:   File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x7f32806be5e0>\n",
      "    Traceback (most recent call last):\n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1324, in __del__\n",
      "    self._shutdown_workers()\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f32806be5e0>\n",
      "\n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1316, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: Traceback (most recent call last):\n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1324, in __del__\n",
      "    can only test a child processself._shutdown_workers()\n",
      "\n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1316, in _shutdown_workers\n",
      "  0%|                                                      | 0/100 [00:00<?, ?it/s]if w.is_alive():\n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "100%|████████████████████████████████████████████| 100/100 [00:25<00:00,  3.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Test Loss: 1.5839471757411956 Avg Test Acc: 0.2521875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test best model\n",
    "model.load_state_dict(best_state)\n",
    "_ = test(model, test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:meta-learning-book]",
   "language": "python",
   "name": "conda-env-meta-learning-book-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
