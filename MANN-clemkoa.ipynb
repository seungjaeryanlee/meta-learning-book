{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00fd7fa0-1caa-403e-bc63-79f078616f99",
   "metadata": {},
   "source": [
    "# MANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9446302-3a8f-46a8-b175-aa1962336848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore InterpolationMode warnings from torchmeta\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce19aaa2-32e6-45b5-99ec-dccbb20486f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f559f694-9efd-40ff-8e78-2cc2e461cf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchmeta.datasets import Omniglot\n",
    "from torchmeta.transforms import Categorical, ClassSplitter, Rotation\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, InterpolationMode\n",
    "from torchmeta.utils.data import BatchMetaDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0796fcda-b77d-4de7-b393-2a4570afacfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed);\n",
    "torch.manual_seed(seed);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2735bcef-fec4-4558-92e0-39703e163840",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcd209b-7825-4565-b8df-91929bb6d549",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### `utils.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7718cf2d-c4e7-45d9-8c22-f4f1cc902027",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roll(t, n):\n",
    "    temp = t.flip(1)\n",
    "    return torch.cat((temp[:, -(n+1):], temp[:, :-(n+1)]), dim=1)\n",
    "\n",
    "\n",
    "def circular_convolution(w, s):\n",
    "    temp_cat = torch.t(torch.cat([roll(s, i) for i in range(w.shape[1])]))\n",
    "    return torch.mm(w, temp_cat)\n",
    "\n",
    "\n",
    "def _convolve(w, s):\n",
    "    \"\"\"Circular convolution implementation.\"\"\"\n",
    "    assert s.size(0) == 3\n",
    "    t = torch.cat([w[-1:], w, w[:1]], dim=0)\n",
    "    c = F.conv1d(t.view(1, 1, -1), s.view(1, 1, -1)).view(-1)\n",
    "    return c\n",
    "\n",
    "\n",
    "def plot_copy_results(target, y, vector_length):\n",
    "    plt.set_cmap('jet')\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(211)\n",
    "    ax1.set_ylabel(\"target\", rotation=0, labelpad=20)\n",
    "    ax1.imshow(torch.t(target.view(-1, vector_length)))\n",
    "    ax1.tick_params(axis=\"both\", which=\"both\", length=0)\n",
    "    ax2 = fig.add_subplot(212)\n",
    "    ax2.set_ylabel(\"output\", rotation=0, labelpad=20)\n",
    "    ax2.imshow(torch.t(y.clone().data.view(-1, vector_length)))\n",
    "    ax2.tick_params(axis=\"both\", which=\"both\", length=0)\n",
    "    plt.setp(ax1.get_xticklabels(), visible=False)\n",
    "    plt.setp(ax1.get_yticklabels(), visible=False)\n",
    "    plt.setp(ax2.get_xticklabels(), visible=False)\n",
    "    plt.setp(ax2.get_yticklabels(), visible=False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c31727c-482f-4d82-89b5-b14560a3a2f4",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### `memory.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0db1337-693e-4b4f-8649-5b49ad82087c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(nn.Module):\n",
    "    def __init__(self, memory_size):\n",
    "        super(Memory, self).__init__()\n",
    "        self._memory_size = memory_size\n",
    "\n",
    "        # Initialize memory bias\n",
    "        initial_state = torch.ones(memory_size) * 1e-6\n",
    "        self.register_buffer('initial_state', initial_state.data)\n",
    "\n",
    "        # Initial read vector is a learnt parameter\n",
    "        self.initial_read = Parameter(torch.randn(1, self._memory_size[1]) * 0.01)\n",
    "\n",
    "    def get_size(self):\n",
    "        return self._memory_size\n",
    "\n",
    "    def reset(self, batch_size):\n",
    "        self.memory = self.initial_state.clone().repeat(batch_size, 1, 1)\n",
    "\n",
    "    def get_initial_read(self, batch_size):\n",
    "        return self.initial_read.clone().repeat(batch_size, 1)\n",
    "\n",
    "    def read(self):\n",
    "        return self.memory\n",
    "\n",
    "    def write(self, w, e, a):\n",
    "        self.memory = self.memory * (1 - torch.matmul(w.unsqueeze(-1), e.unsqueeze(1)))\n",
    "        self.memory = self.memory + torch.matmul(w.unsqueeze(-1), a.unsqueeze(1))\n",
    "        return self.memory\n",
    "\n",
    "    def size(self):\n",
    "        return self._memory_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b0aa9f-aace-4f6a-9733-b903556bdbc3",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### `head.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85f2192c-cd73-4437-af95-f67b7c4c4c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, memory, hidden_size):\n",
    "        super(Head, self).__init__()\n",
    "        self.memory = memory\n",
    "        memory_length, memory_vector_length = memory.get_size()\n",
    "        # (k : vector, beta: scalar, g: scalar, s: vector, gamma: scalar)\n",
    "        self.k_layer = nn.Linear(hidden_size, memory_vector_length)\n",
    "        self.beta_layer = nn.Linear(hidden_size, 1)\n",
    "        self.g_layer = nn.Linear(hidden_size, 1)\n",
    "        self.s_layer = nn.Linear(hidden_size, 3)\n",
    "        self.gamma_layer = nn.Linear(hidden_size, 1)\n",
    "        for layer in [self.k_layer, self.beta_layer, self.g_layer, self.s_layer, self.gamma_layer]:\n",
    "            nn.init.xavier_uniform_(layer.weight, gain=1.4)\n",
    "            nn.init.normal_(layer.bias, std=0.01)\n",
    "\n",
    "        self._initial_state = Parameter(torch.randn(1, self.memory.get_size()[0]) * 1e-5)\n",
    "\n",
    "    def get_initial_state(self, batch_size):\n",
    "        # Softmax to ensure weights are normalized\n",
    "        return F.softmax(self._initial_state, dim=1).repeat(batch_size, 1)\n",
    "\n",
    "    def get_head_weight(self, x, previous_state, memory_read):\n",
    "        k = self.k_layer(x)\n",
    "        beta = F.softplus(self.beta_layer(x))\n",
    "        g = torch.sigmoid(self.g_layer(x))\n",
    "        s = F.softmax(self.s_layer(x), dim=1)\n",
    "        gamma = 1 + F.softplus(self.gamma_layer(x))\n",
    "        # Focusing by content\n",
    "        w_c = F.softmax(beta * F.cosine_similarity(memory_read + 1e-16, k.unsqueeze(1) + 1e-16, dim=-1), dim=1)\n",
    "        # Focusing by location\n",
    "        w_g = g * w_c + (1 - g) * previous_state\n",
    "        w_t = self.shift(w_g, s)\n",
    "        w = w_t ** gamma\n",
    "        w = torch.div(w, torch.sum(w, dim=1).unsqueeze(1) + 1e-16)\n",
    "        return w\n",
    "\n",
    "    def shift(self, w_g, s):\n",
    "        result = w_g.clone()\n",
    "        for b in range(len(w_g)):\n",
    "            result[b] = _convolve(w_g[b], s[b])\n",
    "        return result\n",
    "\n",
    "\n",
    "class ReadHead(Head):\n",
    "    def forward(self, x, previous_state):\n",
    "        memory_read = self.memory.read()\n",
    "        w = self.get_head_weight(x, previous_state, memory_read)\n",
    "        return torch.matmul(w.unsqueeze(1), memory_read).squeeze(1), w\n",
    "\n",
    "\n",
    "class WriteHead(Head):\n",
    "    def __init__(self, memory, hidden_size):\n",
    "        super(WriteHead, self).__init__(memory, hidden_size)\n",
    "        memory_length, memory_vector_length = memory.get_size()\n",
    "        self.e_layer = nn.Linear(hidden_size, memory_vector_length)\n",
    "        self.a_layer = nn.Linear(hidden_size, memory_vector_length)\n",
    "        for layer in [self.e_layer, self.a_layer]:\n",
    "            nn.init.xavier_uniform_(layer.weight, gain=1.4)\n",
    "            nn.init.normal_(layer.bias, std=0.01)\n",
    "\n",
    "    def forward(self, x, previous_state):\n",
    "        memory_read = self.memory.read()\n",
    "        w = self.get_head_weight(x, previous_state, memory_read)\n",
    "        e = torch.sigmoid(self.e_layer(x))\n",
    "        a = self.a_layer(x)\n",
    "\n",
    "        # write to memory (w, memory, e , a)\n",
    "        self.memory.write(w, e, a)\n",
    "        return w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1f7af3-7312-4543-aa41-c5a36a2e2075",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### `controller.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1edc450-259b-4178-8903-d339ddc147f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Controller(nn.Module):\n",
    "    def __init__(self, lstm_controller, vector_length, hidden_size):\n",
    "        super(Controller, self).__init__()\n",
    "        # We allow either a feed-forward network or a LSTM for the controller\n",
    "        self._lstm_controller = lstm_controller\n",
    "        if self._lstm_controller:\n",
    "            self._controller = LSTMController(vector_length, hidden_size)\n",
    "        else:\n",
    "            self._controller = FeedForwardController(vector_length, hidden_size)\n",
    "\n",
    "    def forward(self, x, state):\n",
    "        return self._controller(x, state)\n",
    "\n",
    "    def get_initial_state(self, batch_size):\n",
    "        return self._controller.get_initial_state(batch_size)\n",
    "\n",
    "\n",
    "class LSTMController(nn.Module):\n",
    "    def __init__(self, vector_length, hidden_size):\n",
    "        super(LSTMController, self).__init__()\n",
    "        self.layer = nn.LSTM(input_size=vector_length, hidden_size=hidden_size)\n",
    "        # The hidden state is a learned parameter\n",
    "        self.lstm_h_state = Parameter(torch.randn(1, 1, hidden_size) * 0.05)\n",
    "        self.lstm_c_state = Parameter(torch.randn(1, 1, hidden_size) * 0.05)\n",
    "        for p in self.layer.parameters():\n",
    "            if p.dim() == 1:\n",
    "                nn.init.constant_(p, 0)\n",
    "            else:\n",
    "                stdev = 5 / (np.sqrt(vector_length + hidden_size))\n",
    "                nn.init.uniform_(p, -stdev, stdev)\n",
    "\n",
    "    def forward(self, x, state):\n",
    "        output, state = self.layer(x.unsqueeze(0), state)\n",
    "        return output.squeeze(0), state\n",
    "\n",
    "    def get_initial_state(self, batch_size):\n",
    "        lstm_h = self.lstm_h_state.clone().repeat(1, batch_size, 1)\n",
    "        lstm_c = self.lstm_c_state.clone().repeat(1, batch_size, 1)\n",
    "        return lstm_h, lstm_c\n",
    "\n",
    "\n",
    "class FeedForwardController(nn.Module):\n",
    "    def __init__(self, vector_length, hidden_size):\n",
    "        super(FeedForwardController, self).__init__()\n",
    "        self.layer_1 = nn.Linear(vector_length, hidden_size)\n",
    "        self.layer_2 = nn.Linear(hidden_size, hidden_size)\n",
    "        stdev = 5 / (np.sqrt(vector_length + hidden_size))\n",
    "        nn.init.uniform_(self.layer_1.weight, -stdev, stdev)\n",
    "        nn.init.uniform_(self.layer_2.weight, -stdev, stdev)\n",
    "\n",
    "    def forward(self, x, state):\n",
    "        x1 = F.relu(self.layer_1(x))\n",
    "        output = F.relu(self.layer_2(x1))\n",
    "        return output, state\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        return 0, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b95768-a1ba-4844-af66-77c2d2b5b3fd",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### `ntm.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a63ff5c2-63fb-405e-bcc1-3f83b4458f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTM(nn.Module):\n",
    "    def __init__(self, vector_length, hidden_size, memory_size, lstm_controller=True):\n",
    "        super(NTM, self).__init__()\n",
    "        self.controller = Controller(lstm_controller, vector_length + 1 + memory_size[1], hidden_size)\n",
    "        self.memory = Memory(memory_size)\n",
    "        self.read_head = ReadHead(self.memory, hidden_size)\n",
    "        self.write_head = WriteHead(self.memory, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size + memory_size[1], 5)\n",
    "        nn.init.xavier_uniform_(self.fc.weight, gain=1)\n",
    "        nn.init.normal_(self.fc.bias, std=0.01)\n",
    "\n",
    "    def get_initial_state(self, batch_size):\n",
    "        self.memory.reset(batch_size)\n",
    "        controller_state = self.controller.get_initial_state(batch_size)\n",
    "        read = self.memory.get_initial_read(batch_size)\n",
    "        read_head_state = self.read_head.get_initial_state(batch_size)\n",
    "        write_head_state = self.write_head.get_initial_state(batch_size)\n",
    "        return (read, read_head_state, write_head_state, controller_state)\n",
    "\n",
    "    def forward(self, x, previous_state):\n",
    "        previous_read, previous_read_head_state, previous_write_head_state, previous_controller_state = previous_state\n",
    "        controller_input = torch.cat([x, previous_read], dim=1)\n",
    "        controller_output, controller_state = self.controller(controller_input, previous_controller_state)\n",
    "        # Read\n",
    "        read_head_output, read_head_state = self.read_head(controller_output, previous_read_head_state)\n",
    "        # Write\n",
    "        write_head_state = self.write_head(controller_output, previous_write_head_state)\n",
    "        fc_input = torch.cat((controller_output, read_head_output), dim=1)\n",
    "        state = (read_head_output, read_head_state, write_head_state, controller_state)\n",
    "        return F.softmax(self.fc(fc_input), dim=1), state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf91f2c-dce5-43e7-92fd-427e84792416",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a6a24c4-d0ca-40de-8aef-615951987b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of classes\n",
    "N_WAY = 5\n",
    "# Number of images per class\n",
    "N_SHOT = 5\n",
    "# Size of a minibatch\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "802cc9a5-5f2f-4e08-b48d-270fed9d6ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_LAYER_SIZE = 200\n",
    "MEMORY_SIZE = (128, 40)\n",
    "USE_LSTM_CONTROLLER = True\n",
    "N_EPISODES = 100\n",
    "\n",
    "VECTOR_LENGTH = 20*20 + N_WAY - 1 # TODO: Fix classes to set this properly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e6c566-0a0b-4de0-a224-c5ed62ff3c93",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Omniglot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00894455-5ffa-41ea-9ba1-0d983977a020",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Omniglot(\n",
    "    \"data\",\n",
    "    num_classes_per_task=N_WAY,\n",
    "    transform=Compose([Resize(20), ToTensor()]),\n",
    "    target_transform=Categorical(num_classes=N_WAY),\n",
    "    class_augmentations=[Rotation([90, 180, 270])],\n",
    "    meta_train=True,\n",
    "    download=True,\n",
    ")\n",
    "dataset = ClassSplitter(dataset, shuffle=True, num_train_per_class=N_SHOT, num_test_per_class=N_SHOT)\n",
    "dataloader = BatchMetaDataLoader(dataset, batch_size=BATCH_SIZE, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61ab82b7-7bb9-4b9e-a647-aebed4655e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputs_targets_to_seq(inputs, targets, N_WAY):\n",
    "    X = inputs.flatten(2, 4)\n",
    "    y = F.one_hot(targets, num_classes=N_WAY)\n",
    "    y = torch.cat((torch.zeros(y.shape[0], 1, y.shape[2]), y), dim=1)[:, :-1, :]\n",
    "    seq = torch.cat((X, y), dim=2)\n",
    "    seq = torch.swapaxes(seq, 0, 1)\n",
    "\n",
    "    # Shape: (seq_len, batch, n_features)\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6ea74e-20bd-4c28-9084-a18d15d0a409",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d888660-cf17-4671-a3be-5d14b0f08c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NTM(VECTOR_LENGTH, HIDDEN_LAYER_SIZE, MEMORY_SIZE, USE_LSTM_CONTROLLER)\n",
    "optimizer = optim.RMSprop(model.parameters(), momentum=0.9, alpha=0.95, lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45d06d7-3bb7-4efb-863b-22dac4a969b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc2b369e-2416-4908-a7c3-27f1ded0ada3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_episode(inputs, targets, optimizer, BATCH_SIZE, N_WAY):\n",
    "    seq = inputs_targets_to_seq(inputs, targets, N_WAY)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    state = model.get_initial_state(BATCH_SIZE)\n",
    "    y_out = torch.zeros((len(seq), BATCH_SIZE, N_WAY))\n",
    "    for j, vector in enumerate(seq):\n",
    "        y_out[j], state = model(vector, state)\n",
    "    loss = F.cross_entropy(y_out.permute(1, 2, 0), targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    correct = torch.sum(y_out.permute(1, 2, 0).argmax(dim=1) == targets)\n",
    "    acc = correct.item() / np.prod(train_targets.size())\n",
    "    \n",
    "    return loss.item(), acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96addff1-66eb-4201-8291-7fa7aed2751d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_one_episode(inputs, targets, BATCH_SIZE, N_WAY):\n",
    "    seq = inputs_targets_to_seq(inputs, targets, N_WAY)\n",
    "    state = model.get_initial_state(BATCH_SIZE)\n",
    "    y_out = torch.zeros((len(seq), BATCH_SIZE, N_WAY))\n",
    "    for j, vector in enumerate(seq):\n",
    "        y_out[j], state = model(vector, state)\n",
    "    loss = F.cross_entropy(y_out.permute(1, 2, 0), targets)\n",
    "    correct = torch.sum(y_out.permute(1, 2, 0).argmax(dim=1) == targets)\n",
    "    acc = correct.item() / np.prod(test_targets.size())\n",
    "\n",
    "    return loss.item(), acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e8e365b-870d-4634-933e-94d50ca60f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| EPISODE |      TRAIN      |       TEST      |\n",
      "|         | LOSS   |  ACC   |  LOSS  |  ACC   |\n",
      "|       0 | 1.6084 | 0.1900 | 1.6062 | 0.2400 |\n",
      "|      10 | 1.5710 | 0.4625 | 1.5605 | 0.4875 |\n",
      "|      20 | 1.5162 | 0.5225 | 1.5059 | 0.5825 |\n",
      "|      30 | 1.4755 | 0.5450 | 1.4604 | 0.6000 |\n",
      "|      40 | 1.3890 | 0.6625 | 1.3911 | 0.6750 |\n",
      "|      50 | 1.3262 | 0.7250 | 1.3232 | 0.7175 |\n",
      "|      60 | 1.2815 | 0.7475 | 1.2835 | 0.7450 |\n",
      "|      70 | 1.2395 | 0.7700 | 1.2400 | 0.7725 |\n",
      "|      80 | 1.2120 | 0.7800 | 1.2030 | 0.7900 |\n",
      "|      90 | 1.1760 | 0.8050 | 1.1753 | 0.7975 |\n",
      "|     100 | 1.1544 | 0.8175 | 1.1458 | 0.8075 |\n"
     ]
    }
   ],
   "source": [
    "print(\"| EPISODE |      TRAIN      |       TEST      |\")\n",
    "print(\"|         | LOSS   |  ACC   |  LOSS  |  ACC   |\")\n",
    "train_loss_per_ep, train_acc_per_ep = [], []\n",
    "test_loss_per_ep, test_acc_per_ep = [], []\n",
    "for episode_i, batch in enumerate(dataloader):\n",
    "    if episode_i > N_EPISODES: break\n",
    "\n",
    "    # Train\n",
    "    train_inputs, train_targets = batch[\"train\"]\n",
    "    train_loss, train_acc = train_one_episode(train_inputs, train_targets, optimizer, BATCH_SIZE, N_WAY)\n",
    "    train_loss_per_ep.append(train_loss)\n",
    "    train_acc_per_ep.append(train_acc)\n",
    "\n",
    "    # Test\n",
    "    test_inputs, test_targets = batch[\"test\"]\n",
    "    test_loss, test_acc = test_one_episode(test_inputs, test_targets, BATCH_SIZE, N_WAY)\n",
    "    test_loss_per_ep.append(test_loss)\n",
    "    test_acc_per_ep.append(test_acc)\n",
    "\n",
    "    if episode_i % 10 == 0:\n",
    "        print(f\"| {episode_i:7d} | {train_loss:.4f} | {train_acc:.4f} | {test_loss:.4f} | {test_acc:.4f} |\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:meta-learning-book]",
   "language": "python",
   "name": "conda-env-meta-learning-book-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
