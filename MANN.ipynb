{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "574842a9-f31e-4c93-a449-9ab98c6706be",
   "metadata": {},
   "source": [
    "# MANN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc312eac-c090-4114-82f9-4e83edb28069",
   "metadata": {},
   "source": [
    "This uses NTM from loudinthecloud/pytorch-ntm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c7984d-3ff7-4998-b41e-741b531c4b05",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "221c7bca-9e9f-4c5f-88d1-b183229242a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24a00b89-e1eb-4821-a334-0c56653dcb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmeta.datasets import Omniglot\n",
    "from torchmeta.transforms import Categorical, ClassSplitter, Rotation\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from torchmeta.utils.data import BatchMetaDataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0db28c-1b34-4f12-9a88-e57f3db7cba9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Original Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439d02b0-7fc0-47f9-aeb0-d1899afbb794",
   "metadata": {
    "tags": []
   },
   "source": [
    "### `head.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da1da721-2b56-4a1c-9b5b-52a1026f7718",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"NTM Read and Write Heads.\"\"\"\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def _split_cols(mat, lengths):\n",
    "    \"\"\"Split a 2D matrix to variable length columns.\"\"\"\n",
    "    assert mat.size()[1] == sum(lengths), \"Lengths must be summed to num columns\"\n",
    "    l = np.cumsum([0] + lengths)\n",
    "    results = []\n",
    "    for s, e in zip(l[:-1], l[1:]):\n",
    "        results.append(mat[:, s:e])\n",
    "    return results\n",
    "\n",
    "\n",
    "class NTMHeadBase(nn.Module):\n",
    "    \"\"\"An NTM Read/Write Head.\"\"\"\n",
    "\n",
    "    def __init__(self, memory, controller_size):\n",
    "        \"\"\"Initilize the read/write head.\n",
    "        :param memory: The :class:`NTMMemory` to be addressed by the head.\n",
    "        :param controller_size: The size of the internal representation.\n",
    "        \"\"\"\n",
    "        super(NTMHeadBase, self).__init__()\n",
    "\n",
    "        self.memory = memory\n",
    "        self.N, self.M = memory.size()\n",
    "        self.controller_size = controller_size\n",
    "\n",
    "    def create_new_state(self, batch_size):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def register_parameters(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def is_read_head(self):\n",
    "        return NotImplementedError\n",
    "\n",
    "    def _address_memory(self, k, β, g, s, γ, w_prev):\n",
    "        # Handle Activations\n",
    "        k = k.clone()\n",
    "        β = F.softplus(β)\n",
    "        g = F.sigmoid(g)\n",
    "        s = F.softmax(s, dim=1)\n",
    "        γ = 1 + F.softplus(γ)\n",
    "\n",
    "        w = self.memory.address(k, β, g, s, γ, w_prev)\n",
    "\n",
    "        return w\n",
    "\n",
    "\n",
    "class NTMReadHead(NTMHeadBase):\n",
    "    def __init__(self, memory, controller_size):\n",
    "        super(NTMReadHead, self).__init__(memory, controller_size)\n",
    "\n",
    "        # Corresponding to k, β, g, s, γ sizes from the paper\n",
    "        self.read_lengths = [self.M, 1, 1, 3, 1]\n",
    "        self.fc_read = nn.Linear(controller_size, sum(self.read_lengths))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def create_new_state(self, batch_size):\n",
    "        # The state holds the previous time step address weightings\n",
    "        return torch.zeros(batch_size, self.N)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Initialize the linear layers\n",
    "        nn.init.xavier_uniform_(self.fc_read.weight, gain=1.4)\n",
    "        nn.init.normal_(self.fc_read.bias, std=0.01)\n",
    "\n",
    "    def is_read_head(self):\n",
    "        return True\n",
    "\n",
    "    def forward(self, embeddings, w_prev):\n",
    "        \"\"\"NTMReadHead forward function.\n",
    "        :param embeddings: input representation of the controller.\n",
    "        :param w_prev: previous step state\n",
    "        \"\"\"\n",
    "        o = self.fc_read(embeddings)\n",
    "        k, β, g, s, γ = _split_cols(o, self.read_lengths)\n",
    "\n",
    "        # Read from memory\n",
    "        w = self._address_memory(k, β, g, s, γ, w_prev)\n",
    "        r = self.memory.read(w)\n",
    "\n",
    "        return r.clone(), w.clone()\n",
    "\n",
    "\n",
    "class NTMWriteHead(NTMHeadBase):\n",
    "    def __init__(self, memory, controller_size):\n",
    "        super(NTMWriteHead, self).__init__(memory, controller_size)\n",
    "\n",
    "        # Corresponding to k, β, g, s, γ, e, a sizes from the paper\n",
    "        self.write_lengths = [self.M, 1, 1, 3, 1, self.M, self.M]\n",
    "        self.fc_write = nn.Linear(controller_size, sum(self.write_lengths))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def create_new_state(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.N)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Initialize the linear layers\n",
    "        nn.init.xavier_uniform_(self.fc_write.weight, gain=1.4)\n",
    "        nn.init.normal_(self.fc_write.bias, std=0.01)\n",
    "\n",
    "    def is_read_head(self):\n",
    "        return False\n",
    "\n",
    "    def forward(self, embeddings, w_prev):\n",
    "        \"\"\"NTMWriteHead forward function.\n",
    "        :param embeddings: input representation of the controller.\n",
    "        :param w_prev: previous step state\n",
    "        \"\"\"\n",
    "        o = self.fc_write(embeddings)\n",
    "        k, β, g, s, γ, e, a = _split_cols(o, self.write_lengths)\n",
    "\n",
    "        # e should be in [0, 1]\n",
    "        e = F.sigmoid(e)\n",
    "\n",
    "        # Write to memory\n",
    "        w = self._address_memory(k, β, g, s, γ, w_prev)\n",
    "        self.memory.write(w, e, a)\n",
    "\n",
    "        return w.clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2dfd4a-9382-45ae-ac2c-8ee159de40e9",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### `memory.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4a2d22d-b259-49fa-aebb-34dbe32d6796",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"An NTM's memory implementation.\"\"\"\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def _convolve(w, s):\n",
    "    \"\"\"Circular convolution implementation.\"\"\"\n",
    "    assert s.size(0) == 3\n",
    "    t = torch.cat([w[-1:], w, w[:1]])\n",
    "    c = F.conv1d(t.view(1, 1, -1), s.view(1, 1, -1)).view(-1)\n",
    "    return c\n",
    "\n",
    "\n",
    "class NTMMemory(nn.Module):\n",
    "    \"\"\"Memory bank for NTM.\"\"\"\n",
    "    def __init__(self, N, M):\n",
    "        \"\"\"Initialize the NTM Memory matrix.\n",
    "        The memory's dimensions are (batch_size x N x M).\n",
    "        Each batch has it's own memory matrix.\n",
    "        :param N: Number of rows in the memory.\n",
    "        :param M: Number of columns/features in the memory.\n",
    "        \"\"\"\n",
    "        super(NTMMemory, self).__init__()\n",
    "\n",
    "        self.N = N\n",
    "        self.M = M\n",
    "\n",
    "        # The memory bias allows the heads to learn how to initially address\n",
    "        # memory locations by content\n",
    "        self.register_buffer('mem_bias', torch.Tensor(N, M))\n",
    "\n",
    "        # Initialize memory bias\n",
    "        stdev = 1 / (np.sqrt(N + M))\n",
    "        nn.init.uniform_(self.mem_bias, -stdev, stdev)\n",
    "\n",
    "    def reset(self, batch_size):\n",
    "        \"\"\"Initialize memory from bias, for start-of-sequence.\"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = self.mem_bias.clone().repeat(batch_size, 1, 1)\n",
    "\n",
    "    def size(self):\n",
    "        return self.N, self.M\n",
    "\n",
    "    def read(self, w):\n",
    "        \"\"\"Read from memory (according to section 3.1).\"\"\"\n",
    "        return torch.matmul(w.unsqueeze(1), self.memory).squeeze(1)\n",
    "\n",
    "    def write(self, w, e, a):\n",
    "        \"\"\"write to memory (according to section 3.2).\"\"\"\n",
    "        self.prev_mem = self.memory\n",
    "        self.memory = torch.Tensor(self.batch_size, self.N, self.M)\n",
    "        erase = torch.matmul(w.unsqueeze(-1), e.unsqueeze(1))\n",
    "        add = torch.matmul(w.unsqueeze(-1), a.unsqueeze(1))\n",
    "        self.memory = self.prev_mem * (1 - erase) + add\n",
    "\n",
    "    def address(self, k, β, g, s, γ, w_prev):\n",
    "        \"\"\"NTM Addressing (according to section 3.3).\n",
    "        Returns a softmax weighting over the rows of the memory matrix.\n",
    "        :param k: The key vector.\n",
    "        :param β: The key strength (focus).\n",
    "        :param g: Scalar interpolation gate (with previous weighting).\n",
    "        :param s: Shift weighting.\n",
    "        :param γ: Sharpen weighting scalar.\n",
    "        :param w_prev: The weighting produced in the previous time step.\n",
    "        \"\"\"\n",
    "        # Content focus\n",
    "        wc = self._similarity(k, β)\n",
    "\n",
    "        # Location focus\n",
    "        wg = self._interpolate(w_prev, wc, g)\n",
    "        ŵ = self._shift(wg, s)\n",
    "        w = self._sharpen(ŵ, γ)\n",
    "\n",
    "        return w\n",
    "\n",
    "    def _similarity(self, k, β):\n",
    "        k = k.view(self.batch_size, 1, -1)\n",
    "        w = F.softmax(β * F.cosine_similarity(self.memory + 1e-16, k + 1e-16, dim=-1), dim=1)\n",
    "        return w\n",
    "\n",
    "    def _interpolate(self, w_prev, wc, g):\n",
    "        return g * wc + (1 - g) * w_prev\n",
    "\n",
    "    def _shift(self, wg, s):\n",
    "        result = torch.zeros(wg.size())\n",
    "        for b in range(self.batch_size):\n",
    "            result[b] = _convolve(wg[b], s[b])\n",
    "        return result\n",
    "\n",
    "    def _sharpen(self, ŵ, γ):\n",
    "        w = ŵ ** γ\n",
    "        w = torch.div(w, torch.sum(w, dim=1).view(-1, 1) + 1e-16)\n",
    "        return w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f749f2de-aeba-4692-8036-e50b36c5a5b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### `controller.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88546c2d-e5b6-4a64-8057-4660f1d3bf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"LSTM Controller.\"\"\"\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Parameter\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class LSTMController(nn.Module):\n",
    "    \"\"\"An NTM controller based on LSTM.\"\"\"\n",
    "    def __init__(self, num_inputs, num_outputs, num_layers):\n",
    "        super(LSTMController, self).__init__()\n",
    "\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_outputs = num_outputs\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=num_inputs,\n",
    "                            hidden_size=num_outputs,\n",
    "                            num_layers=num_layers)\n",
    "\n",
    "        # The hidden state is a learned parameter\n",
    "        self.lstm_h_bias = Parameter(torch.randn(self.num_layers, 1, self.num_outputs) * 0.05)\n",
    "        self.lstm_c_bias = Parameter(torch.randn(self.num_layers, 1, self.num_outputs) * 0.05)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def create_new_state(self, batch_size):\n",
    "        # Dimension: (num_layers * num_directions, batch, hidden_size)\n",
    "        lstm_h = self.lstm_h_bias.clone().repeat(1, batch_size, 1)\n",
    "        lstm_c = self.lstm_c_bias.clone().repeat(1, batch_size, 1)\n",
    "        return lstm_h, lstm_c\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for p in self.lstm.parameters():\n",
    "            if p.dim() == 1:\n",
    "                nn.init.constant_(p, 0)\n",
    "            else:\n",
    "                stdev = 5 / (np.sqrt(self.num_inputs +  self.num_outputs))\n",
    "                nn.init.uniform_(p, -stdev, stdev)\n",
    "\n",
    "    def size(self):\n",
    "        return self.num_inputs, self.num_outputs\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        x = x.unsqueeze(0)\n",
    "        outp, state = self.lstm(x, prev_state)\n",
    "        return outp.squeeze(0), state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c399c36f-6c8f-424d-a41a-dd35c1bb03d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### `ntm.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fc6f0c2-e60c-4700-a704-dcc38c23bacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class NTM(nn.Module):\n",
    "    \"\"\"A Neural Turing Machine.\"\"\"\n",
    "    def __init__(self, num_inputs, num_outputs, controller, memory, heads):\n",
    "        \"\"\"Initialize the NTM.\n",
    "        :param num_inputs: External input size.\n",
    "        :param num_outputs: External output size.\n",
    "        :param controller: :class:`LSTMController`\n",
    "        :param memory: :class:`NTMMemory`\n",
    "        :param heads: list of :class:`NTMReadHead` or :class:`NTMWriteHead`\n",
    "        Note: This design allows the flexibility of using any number of read and\n",
    "              write heads independently, also, the order by which the heads are\n",
    "              called in controlled by the user (order in list)\n",
    "        \"\"\"\n",
    "        super(NTM, self).__init__()\n",
    "\n",
    "        # Save arguments\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_outputs = num_outputs\n",
    "        self.controller = controller\n",
    "        self.memory = memory\n",
    "        self.heads = heads\n",
    "\n",
    "        self.N, self.M = memory.size()\n",
    "        _, self.controller_size = controller.size()\n",
    "\n",
    "        # Initialize the initial previous read values to random biases\n",
    "        self.num_read_heads = 0\n",
    "        self.init_r = []\n",
    "        for head in heads:\n",
    "            if head.is_read_head():\n",
    "                init_r_bias = torch.randn(1, self.M) * 0.01\n",
    "                self.register_buffer(\"read{}_bias\".format(self.num_read_heads), init_r_bias.data)\n",
    "                self.init_r += [init_r_bias]\n",
    "                self.num_read_heads += 1\n",
    "\n",
    "        assert self.num_read_heads > 0, \"heads list must contain at least a single read head\"\n",
    "\n",
    "        # Initialize a fully connected layer to produce the actual output:\n",
    "        #   [controller_output; previous_reads ] -> output\n",
    "        self.fc = nn.Linear(self.controller_size + self.num_read_heads * self.M, num_outputs)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def create_new_state(self, batch_size):\n",
    "        init_r = [r.clone().repeat(batch_size, 1) for r in self.init_r]\n",
    "        controller_state = self.controller.create_new_state(batch_size)\n",
    "        heads_state = [head.create_new_state(batch_size) for head in self.heads]\n",
    "\n",
    "        return init_r, controller_state, heads_state\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Initialize the linear layer\n",
    "        nn.init.xavier_uniform_(self.fc.weight, gain=1)\n",
    "        nn.init.normal_(self.fc.bias, std=0.01)\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        \"\"\"NTM forward function.\n",
    "        :param x: input vector (batch_size x num_inputs)\n",
    "        :param prev_state: The previous state of the NTM\n",
    "        \"\"\"\n",
    "        # Unpack the previous state\n",
    "        prev_reads, prev_controller_state, prev_heads_states = prev_state\n",
    "\n",
    "        # Use the controller to get an embeddings\n",
    "        inp = torch.cat([x] + prev_reads, dim=1)\n",
    "#         inp = torch.zeros_like(inp) # DEBUG - ERROR\n",
    "#         prev_controller_state = torch.zeros_like(prev_controller_state[0]), prev_controller_state[1] # DEBUG - ERROR\n",
    "#         prev_controller_state = prev_controller_state[0], torch.zeros_like(prev_controller_state[1]) # DEBUG - ERROR\n",
    "#         prev_controller_state = prev_controller_state[0].clone(), prev_controller_state[1].clone() # DEBUG - ERROR\n",
    "        prev_controller_state = prev_controller_state[0].detach(), prev_controller_state[1].detach() # DEBUG - FINE\n",
    "#         prev_controller_state = torch.zeros_like(prev_controller_state[0]), torch.zeros_like(prev_controller_state[1]) # DEBUG - FINE\n",
    "        controller_outp, controller_state = self.controller(inp, prev_controller_state)\n",
    "\n",
    "        # Read/Write from the list of heads\n",
    "        reads = []\n",
    "        heads_states = []\n",
    "        for head, prev_head_state in zip(self.heads, prev_heads_states):\n",
    "            if head.is_read_head():\n",
    "                r, head_state = head(controller_outp, prev_head_state)\n",
    "#                 prev_head_state = torch.zeros_like(prev_head_state[0]), prev_head_state[1] # DEBUG - ERROR\n",
    "                r = torch.zeros_like(r) # DEBUG - FINE\n",
    "                reads.append(r)\n",
    "            else:\n",
    "                head_state = head(controller_outp, prev_head_state)\n",
    "            heads_states.append(head_state)\n",
    "\n",
    "        # Generate Output\n",
    "        inp2 = torch.cat([controller_outp] + reads, dim=1)\n",
    "        o = F.sigmoid(self.fc(inp2))\n",
    "\n",
    "\n",
    "        # Pack the current state\n",
    "        state = (reads, controller_state, heads_states)\n",
    "\n",
    "        return o, state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0db392-e49c-464d-8156-b1a07a2931cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### `aio.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43ee6e01-b88a-40fa-a6ea-f0dc79e8c406",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"All in one NTM. Encapsulation of all components.\"\"\"\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class EncapsulatedNTM(nn.Module):\n",
    "\n",
    "    def __init__(self, num_inputs, num_outputs,\n",
    "                 controller_size, controller_layers, num_heads, N, M):\n",
    "        \"\"\"Initialize an EncapsulatedNTM.\n",
    "        :param num_inputs: External number of inputs.\n",
    "        :param num_outputs: External number of outputs.\n",
    "        :param controller_size: The size of the internal representation.\n",
    "        :param controller_layers: Controller number of layers.\n",
    "        :param num_heads: Number of heads.\n",
    "        :param N: Number of rows in the memory bank.\n",
    "        :param M: Number of cols/features in the memory bank.\n",
    "        \"\"\"\n",
    "        super(EncapsulatedNTM, self).__init__()\n",
    "\n",
    "        # Save args\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_outputs = num_outputs\n",
    "        self.controller_size = controller_size\n",
    "        self.controller_layers = controller_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.N = N\n",
    "        self.M = M\n",
    "\n",
    "        # Create the NTM components\n",
    "        memory = NTMMemory(N, M)\n",
    "        controller = LSTMController(num_inputs + M*num_heads, controller_size, controller_layers)\n",
    "        heads = nn.ModuleList([])\n",
    "        for i in range(num_heads):\n",
    "            heads += [\n",
    "                NTMReadHead(memory, controller_size),\n",
    "                NTMWriteHead(memory, controller_size)\n",
    "            ]\n",
    "\n",
    "        self.ntm = NTM(num_inputs, num_outputs, controller, memory, heads)\n",
    "        self.memory = memory\n",
    "\n",
    "    def init_sequence(self, batch_size):\n",
    "        \"\"\"Initializing the state.\"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.memory.reset(batch_size)\n",
    "        self.previous_state = self.ntm.create_new_state(batch_size)\n",
    "\n",
    "    def forward(self, x=None):\n",
    "        if x is None:\n",
    "            x = torch.zeros(self.batch_size, self.num_inputs)\n",
    "\n",
    "        o, self.previous_state = self.ntm(x, self.previous_state)\n",
    "        return o, self.previous_state\n",
    "\n",
    "    def calculate_num_params(self):\n",
    "        \"\"\"Returns the total number of parameters.\"\"\"\n",
    "        num_params = 0\n",
    "        for p in self.parameters():\n",
    "            num_params += p.data.view(-1).size(0)\n",
    "        return num_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511f2605-cca0-4f82-881e-16eb635e67b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### `train.py` (Partial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdd0906e-9f19-470f-b664-217ee79dbe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(net, criterion, optimizer, X, Y):\n",
    "    \"\"\"Trains a single batch.\"\"\"\n",
    "    optimizer.zero_grad()\n",
    "    inp_seq_len = X.size(0)\n",
    "    outp_seq_len, batch_size, _ = Y.size()\n",
    "\n",
    "    # New sequence\n",
    "    net.init_sequence(batch_size)\n",
    "\n",
    "    # Feed the sequence + delimiter\n",
    "    for i in range(inp_seq_len):\n",
    "        net(X[i])\n",
    "\n",
    "    # Read the output (no input given)\n",
    "    y_out = torch.zeros(Y.size())\n",
    "    for i in range(outp_seq_len):\n",
    "        y_out[i], _ = net()\n",
    "\n",
    "    loss = criterion(y_out, Y)\n",
    "    loss.backward()\n",
    "    clip_grads(net)\n",
    "    optimizer.step()\n",
    "\n",
    "    y_out_binarized = y_out.clone().data\n",
    "    y_out_binarized.apply_(lambda x: 0 if x < 0.5 else 1)\n",
    "\n",
    "    # The cost is the number of error bits per sequence\n",
    "    cost = torch.sum(torch.abs(y_out_binarized - Y.data))\n",
    "\n",
    "    return loss.item(), cost.item() / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "987977ea-e115-4f22-8195-97f9deffeb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, args):\n",
    "    num_batches = model.params.num_batches\n",
    "    batch_size = model.params.batch_size\n",
    "\n",
    "    LOGGER.info(\"Training model for %d batches (batch_size=%d)...\",\n",
    "                num_batches, batch_size)\n",
    "\n",
    "    losses = []\n",
    "    costs = []\n",
    "    seq_lengths = []\n",
    "    start_ms = get_ms()\n",
    "\n",
    "    for batch_num, x, y in model.dataloader:\n",
    "        loss, cost = train_batch(model.net, model.criterion, model.optimizer, x, y)\n",
    "        losses += [loss]\n",
    "        costs += [cost]\n",
    "        seq_lengths += [y.size(0)]\n",
    "\n",
    "        # Update the progress bar\n",
    "        progress_bar(batch_num, args.report_interval, loss)\n",
    "\n",
    "        # Report\n",
    "        if batch_num % args.report_interval == 0:\n",
    "            mean_loss = np.array(losses[-args.report_interval:]).mean()\n",
    "            mean_cost = np.array(costs[-args.report_interval:]).mean()\n",
    "            mean_time = int(((get_ms() - start_ms) / args.report_interval) / batch_size)\n",
    "            progress_clean()\n",
    "            LOGGER.info(\"Batch %d Loss: %.6f Cost: %.2f Time: %d ms/sequence\",\n",
    "                        batch_num, mean_loss, mean_cost, mean_time)\n",
    "            start_ms = get_ms()\n",
    "\n",
    "        # Checkpoint\n",
    "        if (args.checkpoint_interval != 0) and (batch_num % args.checkpoint_interval == 0):\n",
    "            save_checkpoint(model.net, model.params.name, args,\n",
    "                            batch_num, losses, costs, seq_lengths)\n",
    "\n",
    "    LOGGER.info(\"Done training.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e95ac87-194f-487c-9500-3d211b259baa",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e47f50e4-1b48-4923-8a66-c7e74a48e560",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Omniglot(\n",
    "    \"data\",\n",
    "    # Number of ways\n",
    "    num_classes_per_task=5,\n",
    "    # Resize the images to 20x20 and converts them to PyTorch tensors (from Torchvision)\n",
    "    transform=Compose([Resize(20), ToTensor()]),\n",
    "    # Transform the labels to integers (e.g. (\"Glagolitic/character01\", \"Sanskrit/character14\", ...) to (0, 1, ...))\n",
    "    target_transform=Categorical(num_classes=5),\n",
    "    # Creates new virtual classes with rotated versions of the images (from Santoro et al., 2016)\n",
    "    class_augmentations=[Rotation([90, 180, 270])],\n",
    "    meta_train=True,\n",
    "    download=True,\n",
    ")\n",
    "dataset = ClassSplitter(dataset, shuffle=True, num_train_per_class=5, num_test_per_class=5)\n",
    "dataloader = BatchMetaDataLoader(dataset, batch_size=16, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a44a77-a9d4-4a52-9f90-b343a9d22484",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12bea368-ead4-455d-a3d7-97913edb27fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_grads(net):\n",
    "    \"\"\"Gradient clipping to the range [10, 10].\"\"\"\n",
    "    parameters = list(filter(lambda p: p.grad is not None, net.parameters()))\n",
    "    for p in parameters:\n",
    "        p.grad.data.clamp_(-10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa8b366e-1601-45dd-854e-c015529a024a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputs_targets_to_seq(inputs, targets):\n",
    "    X = inputs.flatten(2, 4)\n",
    "    y = F.one_hot(targets, num_classes=5)\n",
    "    y = torch.cat((torch.zeros(y.shape[0], 1, y.shape[2]), y), dim=1)[:, :-1, :]\n",
    "    seq = torch.cat((X, y), dim=2)\n",
    "    seq = torch.swapaxes(seq, 0, 1)\n",
    "\n",
    "    # Shape: (seq_len, batch, n_features)\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc46a24a-820b-4184-9ded-3986f2463fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torchvision/transforms/functional.py:942: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torchvision/transforms/functional.py:942: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torchvision/transforms/functional.py:942: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torchvision/transforms/functional.py:942: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25, 16, 405])\n"
     ]
    }
   ],
   "source": [
    "for episode_i, batch in enumerate(dataloader):\n",
    "    train_inputs, train_targets = batch[\"train\"]\n",
    "    seq = inputs_targets_to_seq(train_inputs, train_targets)\n",
    "    print(seq.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5b737c-6dff-44ea-be84-65f5c5f205db",
   "metadata": {},
   "source": [
    "## Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "136c0f4f-2826-4bcd-ac0e-a8d99832cd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_seq_len = 25\n",
    "outp_seq_len = 25\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1bf1b80-df9f-414d-b476-c4b4d4e70d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "entm = EncapsulatedNTM(num_inputs=20*20+5, num_outputs=5, controller_size=200, controller_layers=4, num_heads=4, N=40, M=128)\n",
    "entm.init_sequence(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8281877-c6a7-4993-bb06-46424e1f7d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(entm.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aefbc4a4-509f-4a7f-8549-503520a9fd1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f90fda87970>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01ce474f-6caf-476b-ada4-da90af1fe7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torchvision/transforms/functional.py:942: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torchvision/transforms/functional.py:942: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torchvision/transforms/functional.py:942: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torchvision/transforms/functional.py:942: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6089913845062256\n",
      "1.6092240810394287\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22019/1652047291.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0moutp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_seq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 5 = n_classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_seq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0moutp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Compute loss and backpropagate gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_22019/321889709.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprevious_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mntm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprevious_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprevious_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_22019/1991382455.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, prev_state)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mreads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mhead_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontroller_outp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_head_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0mheads_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_22019/2680779264.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, embeddings, w_prev)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m# Write to memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_address_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mβ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mγ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_prev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_22019/2680779264.py\u001b[0m in \u001b[0;36m_address_memory\u001b[0;34m(self, k, β, g, s, γ, w_prev)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mγ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftplus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mγ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mβ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mγ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_prev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_22019/3271816083.py\u001b[0m in \u001b[0;36maddress\u001b[0;34m(self, k, β, g, s, γ, w_prev)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# Location focus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mwg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mŵ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sharpen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mŵ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mγ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_22019/3271816083.py\u001b[0m in \u001b[0;36m_interpolate\u001b[0;34m(self, w_prev, wc, g)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_interpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mwc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw_prev\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_shift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/meta-learning-book/lib/python3.9/traceback.py\u001b[0m in \u001b[0;36mformat_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mformat_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/meta-learning-book/lib/python3.9/traceback.py\u001b[0m in \u001b[0;36mextract_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0mstack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStackSummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwalk_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m     \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/meta-learning-book/lib/python3.9/traceback.py\u001b[0m in \u001b[0;36mextract\u001b[0;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlookup_lines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/meta-learning-book/lib/python3.9/traceback.py\u001b[0m in \u001b[0;36mline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_line\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinecache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlineno\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/meta-learning-book/lib/python3.9/linecache.py\u001b[0m in \u001b[0;36mgetline\u001b[0;34m(filename, lineno, module_globals)\u001b[0m\n\u001b[1;32m     28\u001b[0m     Update the cache if it doesn't contain an entry for this file already.\"\"\"\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_globals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mlineno\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlineno\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/meta-learning-book/lib/python3.9/linecache.py\u001b[0m in \u001b[0;36mgetlines\u001b[0;34m(filename, module_globals)\u001b[0m\n\u001b[1;32m     38\u001b[0m     Update the cache if it doesn't contain an entry for this file already.\"\"\"\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for episode_i, batch in enumerate(dataloader):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Convert torchmeta batch to MANN input sequence\n",
    "    train_inputs, train_targets = batch[\"train\"]\n",
    "    seq = inputs_targets_to_seq(train_inputs, train_targets)\n",
    "\n",
    "    # Feed the sequence to ENTM\n",
    "    outp = torch.zeros((inp_seq_len, batch_size, 5)) # 5 = n_classes\n",
    "    for i in range(inp_seq_len):\n",
    "        outp[i], _ = entm(seq[i])\n",
    "\n",
    "    # Compute loss and backpropagate gradients\n",
    "    prediction = outp.permute(1, 2, 0)\n",
    "    target = train_targets\n",
    "    loss = criterion(prediction, target)\n",
    "    loss.backward()\n",
    "    clip_grads(entm)\n",
    "    optimizer.step()\n",
    "    print(loss.item())\n",
    "\n",
    "    if episode_i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8d4631-2a60-4f8e-8f53-d2c56dcc6e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:meta-learning-book]",
   "language": "python",
   "name": "conda-env-meta-learning-book-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
