{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Augmented Neural Network using Omniglot Dataset\n",
    "\n",
    "\n",
    "In this tutorial, we will do following things step by step:\n",
    "1. Data Preprocessing: Creating Pairs.\n",
    "2. Create a Memory Augmented Enural Network\n",
    "3. Train it using Omniglot dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo apt-get install python3-opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /home/seungjaeryanlee/anaconda3/lib/python3.8/site-packages (4.5.3.56)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/seungjaeryanlee/anaconda3/lib/python3.8/site-packages (from opencv-python) (1.19.5)\n",
      "Requirement already satisfied: Pillow in /home/seungjaeryanlee/anaconda3/lib/python3.8/site-packages (8.3.1)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Move to requirements.txt\n",
    "# !pip install scipy==1.1.0\n",
    "!pip install opencv-python\n",
    "!pip install Pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "##### Step1: Lets first import all libraries needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils as ut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "#### Step2 : Load Data\n",
    "\n",
    "We are reading images from two folders named 'image_background', 'image_evaluation' defined in 'data' directory\n",
    "\n",
    "Dataset is divided into 1423 charcters images for training and rest for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1623 total character classes\n",
      "1423 characters assigned for training, 200 characters assigned for validation\n"
     ]
    }
   ],
   "source": [
    "width = 20\n",
    "\n",
    "# gather data paths\n",
    "subfolds = ut.extend_children('data','')\n",
    "datafolds = [subfolds[0],subfolds[1]]\n",
    "alphabets = ut.extend_generation(datafolds,'')\n",
    "charpaths = ut.extend_generation(alphabets,'')\n",
    "chars_dataset = [v.split('/')[2]+'/'+v.split('/')[3] for v in charpaths]\n",
    "\n",
    "# index-value conversion dictionaries for character set\n",
    "i2v = {i:v for i, v in enumerate(chars_dataset)}\n",
    "v2i = {v:i for i, v in enumerate(chars_dataset)}\n",
    "\n",
    "# get size of dataset\n",
    "mc_dataset = len(charpaths)\n",
    "print(mc_dataset,'total character classes')\n",
    "\n",
    "# train split\n",
    "mc_train = 1423\n",
    "chars_train = chars_dataset[:mc_train]\n",
    "classes_train = [v2i[v] for v in chars_train]\n",
    "\n",
    "# validation split\n",
    "mc_val = mc_dataset-mc_train\n",
    "chars_val = chars_dataset[-mc_val:]\n",
    "classes_val = [v2i[v] for v in chars_val]\n",
    "\n",
    "\n",
    "\n",
    "print('%s characters assigned for training, %s characters assigned for validation'%(mc_train,mc_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 3: Read Data\n",
    "\n",
    "All character images are read and saved in the imgs_dataset variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1623 character folders loaded\n",
      "200/1623 character folders loaded\n",
      "400/1623 character folders loaded\n",
      "600/1623 character folders loaded\n",
      "800/1623 character folders loaded\n",
      "1000/1623 character folders loaded\n",
      "1200/1623 character folders loaded\n",
      "1400/1623 character folders loaded\n",
      "1600/1623 character folders loaded\n"
     ]
    }
   ],
   "source": [
    "# load images from character paths\n",
    "imgs_dataset = []\n",
    "for i, charfold in enumerate(charpaths):\n",
    "    if i%200==0:\n",
    "        print('%s/%s character folders loaded'%(i,mc_dataset))\n",
    "    imgs_dataset.append([ ut.load_image(imgpath,(width,width))/127.5-1 for imgpath in ut.extend_children(charfold,'.png') ] )\n",
    "# access imgs_dataset [ character index ] [ sample index ] [ row,col ]\n",
    "\n",
    "# split images between train and validation sets\n",
    "imgs_train = imgs_dataset[:mc_train]\n",
    "imgs_val = imgs_dataset[-mc_val:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Let's Initialize our Hyper-paramters\n",
    "\n",
    "We define all the hyper-parameters and dimensions of all the variables here.\n",
    "* n_classes is 5 (in each batch we have 5 types of characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_classes = 5\n",
    "memory_size = 128 # number of features per entry\n",
    "memory_dim = 40 # number of entries in memory\n",
    "learning_rate = 1e-3\n",
    "batch_size = 16\n",
    "\n",
    "n_inputs = width*width+n_classes # input to LSTM cell.\n",
    "n_hnodes = 200 # LSTM cell size\n",
    "n_outputs = n_classes\n",
    "mem_size = memory_size # number of rows in external memory \n",
    "mem_dim = memory_dim # number of columns in external memory\n",
    "n_reads = 4 # number of heads. \n",
    "\n",
    "n_xh = n_inputs+n_hnodes # inputs to LSTM cell (previous state + input image dim)\n",
    "n_rd = n_reads*mem_dim\n",
    "n_hr = n_hnodes+n_rd\n",
    "gamma = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Let's Create all Trainable parameters for network and initialize them.\n",
    "\n",
    "We define all the hyper-parameters and dimensions of all the variables here.\n",
    "\n",
    "This defines three types of weights:\n",
    "* LSTM weights (we use LSTM controller, a feed-forward controller could also be used)\n",
    "* Read and Write key weights\n",
    "* Output fully connected layer weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LSTM gates (4 of them: weights and biases for each)\n",
    "W_gf = torch.Tensor(n_xh, n_hnodes).uniform_(-1., 1.).requires_grad_()\n",
    "b_gf = torch.Tensor(n_hnodes).uniform_(-1., 1.).requires_grad_()\n",
    "W_gi = torch.Tensor(n_xh,n_hnodes).uniform_(-1., 1.).requires_grad_()\n",
    "b_gi = torch.Tensor(n_hnodes).uniform_(-1., 1.).requires_grad_()\n",
    "W_go = torch.Tensor(n_xh,n_hnodes).uniform_(-1., 1.).requires_grad_()\n",
    "b_go = torch.Tensor(n_hnodes).uniform_(-1., 1.).requires_grad_()\n",
    "W_u = torch.Tensor(n_xh,n_hnodes).uniform_(-1., 1.).requires_grad_()\n",
    "b_u = torch.Tensor(n_hnodes).uniform_(-1., 1.).requires_grad_()\n",
    "# Controller Weights\n",
    "W_kr = torch.Tensor(n_hnodes,n_rd).uniform_(-1., 1.).requires_grad_()\n",
    "b_kr = torch.Tensor(n_rd).uniform_(-1., 1.).requires_grad_()\n",
    "W_kw = torch.Tensor(n_hnodes,n_rd).uniform_(-1., 1.).requires_grad_()\n",
    "b_kw = torch.Tensor(n_rd).uniform_(-1., 1.).requires_grad_()\n",
    "W_ga = torch.Tensor(n_hnodes,n_reads).uniform_(-1., 1.).requires_grad_()\n",
    "b_ga = torch.Tensor(n_reads).uniform_(-1., 1.).requires_grad_()\n",
    "# logit weights\n",
    "W_o = torch.Tensor(n_hr,n_outputs).uniform_(-1., 1.).requires_grad_()\n",
    "b_o = torch.Tensor(n_outputs).uniform_(-1., 1.).requires_grad_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Define Model\n",
    "\n",
    "The next cell defines the forward pass of the network. \n",
    "It works as follows:\n",
    "* the net() function receives input X which is sliced along dim 0 which is the time dimension\n",
    "* we sequentially process each time step in run_one_step() function and collect state vectors of each step's output\n",
    "* The predictions are made at each time step with fully connected output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_state0():\n",
    "    # memory variables (not trainable.)\n",
    "    # initialize memory and LSTM states with zero. \n",
    "    return(\n",
    "        torch.FloatTensor(1e-6*np.random.rand(batch_size,mem_size,mem_dim)),\n",
    "        torch.FloatTensor(np.zeros((batch_size,n_hnodes))),\n",
    "        torch.FloatTensor(np.zeros((batch_size,n_hnodes))),\n",
    "        torch.FloatTensor(np.zeros((batch_size,mem_size))),\n",
    "        torch.FloatTensor(np.zeros((batch_size,n_reads,mem_size))),\n",
    "        torch.FloatTensor(np.zeros((batch_size,n_reads,mem_dim))),\n",
    "    )\n",
    "\n",
    "def run_one_step(X_t, state):\n",
    "    # Run one step of the episode.\n",
    "    M_tm1, h_tm1, c_tm1, wu_tm1, wr_tm1, r_tm1 = state\n",
    "    X_t_r = X_t.view(-1,n_inputs)\n",
    "    xh = torch.cat((X_t_r,h_tm1),1)\n",
    "    gf = torch.sigmoid(torch.matmul(xh,W_gf) + b_gf)\n",
    "    gi = torch.sigmoid(torch.matmul(xh,W_gi) + b_gi)\n",
    "    go = torch.sigmoid(torch.matmul(xh,W_go) + b_go)\n",
    "    u_t = torch.tanh(torch.matmul(xh,W_u) + b_u)\n",
    "    c_t = c_tm1*gf + u_t*gi\n",
    "    h_t = c_t*go\n",
    "    kr_t = torch.tanh(torch.matmul(c_t,W_kr) + b_kr).view(batch_size,n_reads,mem_dim)\n",
    "    kw_t = torch.tanh(torch.matmul(c_t,W_kw) + b_kw).view(batch_size,n_reads,mem_dim)\n",
    "    k_norm = torch.norm(kr_t, dim=2, keepdim=True)\n",
    "    m_norm = torch.norm(M_tm1, dim=2, keepdim=True)\n",
    "    inner_prod = torch.matmul(kr_t, M_tm1.permute(0,2,1))\n",
    "    norm_prod = torch.matmul(k_norm, m_norm.permute(0,2,1))\n",
    "    wr_t = F.softmax(inner_prod/norm_prod)\n",
    "    wu_1 = wu_tm1*gamma + torch.sum(wr_t, dim=1)\n",
    "    r_t = torch.matmul(wr_t,M_tm1)\n",
    "    ga = torch.unsqueeze(torch.sigmoid(torch.matmul(h_t,W_ga)+b_ga),2)\n",
    "    _, wlu_inds = torch.topk(-1*wu_1,k=n_reads)\n",
    "    wlu_t = torch.sum(F.one_hot(wlu_inds, mem_size).type(torch.FloatTensor),dim=1,keepdim=True)\n",
    "    ww_t = wr_t*ga + wlu_t*(1-ga)\n",
    "    wu_t = wu_1 + torch.sum(ww_t, dim=1)\n",
    "    M_1 = M_tm1 * (-1*wlu_t).permute(0,2,1)\n",
    "    M_t = M_1 + torch.matmul(ww_t.permute(0,2,1), kw_t)\n",
    "    st8_t = (M_t, h_t, c_t, wu_t, wr_t, r_t)\n",
    "    return st8_t\n",
    "    \n",
    "\n",
    "    \n",
    "def net(X=None, y=None, batch_size=16):\n",
    "    # X is of shape (batch_size, None, width, width)\n",
    "    a = np.arange(100*batch_size*405).reshape((100,batch_size,405)).astype(np.float32)\n",
    "    X = torch.from_numpy(a)\n",
    "    \n",
    "    state0 = get_state0()\n",
    "    curr_state = state0\n",
    "    \n",
    "    # Collect output of state vectors in each time step.\n",
    "    M_f = []\n",
    "    h_f = []\n",
    "    c_f = []\n",
    "    wu_f = []\n",
    "    wr_f = []\n",
    "    r_f = []\n",
    "    \n",
    "    for i in range(X.shape[0]):\n",
    "        curr_state = run_one_step(X[i], curr_state)\n",
    "        M_f.append(curr_state[0])\n",
    "        h_f.append(curr_state[1])\n",
    "        c_f.append(curr_state[2])\n",
    "        wu_f.append(curr_state[3])\n",
    "        wr_f.append(curr_state[4])\n",
    "        r_f.append(curr_state[5])\n",
    "        \n",
    "    M_f = torch.stack(M_f)\n",
    "    h_f = torch.stack(h_f)\n",
    "    c_f = torch.stack(c_f)\n",
    "    wu_f = torch.stack(wu_f)\n",
    "    wr_f = torch.stack(wr_f)\n",
    "    r_f = torch.stack(r_f)\n",
    "\n",
    "    hr = torch.cat((h_f, r_f.view(-1,batch_size,n_rd)),2)\n",
    "    o_f = torch.tensordot(hr,W_o,1)+b_o\n",
    "    return (M_f, h_f, c_f, wu_f, wr_f, r_f, o_f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 7: Initialize optimizer and criteria for  training\n",
    "\n",
    "We use Adam optimizer on cross entropy loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam([W_gf, b_gf, W_gi, b_gi, W_go, b_go, W_u, b_u, W_kr,b_kr, W_kw, b_kw,\n",
    "                      W_ga, b_ga, W_o, b_o], lr=learning_rate)\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare graph input\n",
    "\n",
    "Y-labels predicted at each time step are appended to input at next time step for better signal.\n",
    "* During training, we have access to y_labels at time step. Shift the y_labels by one time step and append this to the input to prepare input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_graph_input(X_train, y_train):\n",
    "    X = np.transpose(X_train.reshape(batch_size,-1,width*width),(1,0,2))\n",
    "    y_labels = np.transpose(y_train, (1,0,2))\n",
    "    y_labels_shifted = np.concatenate((np.zeros((1,batch_size,n_classes)), y_labels[:-1,:]),0)\n",
    "    X = np.concatenate((X,y_labels_shifted),-1)\n",
    "    y = np.argmax(y_labels, -1)\n",
    "    return torch.from_numpy(X), torch.from_numpy(y)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 starting..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-4450f92e46be>:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  wr_t = F.softmax(inner_prod/norm_prod)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / Batch (1/17) / Loss 198.285416\n",
      "Epoch 1 / Batch (2/17) / Loss 185.619492\n",
      "Epoch 1 / Batch (3/17) / Loss 184.597916\n",
      "Epoch 1 / Batch (4/17) / Loss 183.269958\n",
      "Epoch 1 / Batch (5/17) / Loss 179.000671\n",
      "Epoch 1 / Batch (6/17) / Loss 171.027740\n",
      "Epoch 1 / Batch (7/17) / Loss 169.361588\n",
      "Epoch 1 / Batch (8/17) / Loss 166.101196\n",
      "Epoch 1 / Batch (9/17) / Loss 163.172104\n",
      "Epoch 1 / Batch (10/17) / Loss 163.798676\n",
      "Epoch 1 / Batch (11/17) / Loss 158.648010\n",
      "Epoch 1 / Batch (12/17) / Loss 152.804016\n",
      "Epoch 1 / Batch (13/17) / Loss 156.739975\n",
      "Epoch 1 / Batch (14/17) / Loss 153.878540\n",
      "Epoch 1 / Batch (15/17) / Loss 153.459106\n",
      "Epoch 1 / Batch (16/17) / Loss 152.503510\n",
      "Epoch 1 / Batch (17/17) / Loss 147.269653\n",
      "Epoch 1 complete, 7.773889780044556 seconds elapsed\n",
      "Epoch 2 starting..\n",
      "Epoch 2 complete, 7.761071443557739 seconds elapsed\n",
      "Epoch 3 starting..\n",
      "Epoch 3 complete, 7.5476274490356445 seconds elapsed\n",
      "Epoch 4 starting..\n",
      "Epoch 4 complete, 7.190585374832153 seconds elapsed\n",
      "Epoch 5 starting..\n",
      "Epoch 5 complete, 7.189485311508179 seconds elapsed\n",
      "Epoch 6 starting..\n",
      "Epoch 6 complete, 7.240983009338379 seconds elapsed\n",
      "Epoch 7 starting..\n",
      "Epoch 7 complete, 7.553049802780151 seconds elapsed\n",
      "Epoch 8 starting..\n",
      "Epoch 8 complete, 7.278620958328247 seconds elapsed\n",
      "Epoch 9 starting..\n",
      "Epoch 9 complete, 7.258403539657593 seconds elapsed\n",
      "Epoch 10 starting..\n",
      "Epoch 10 complete, 7.287006378173828 seconds elapsed\n",
      "Epoch 11 starting..\n",
      "Epoch 11 / Batch (1/17) / Loss 9.033299\n",
      "Epoch 11 / Batch (2/17) / Loss 8.808132\n",
      "Epoch 11 / Batch (3/17) / Loss 7.854129\n",
      "Epoch 11 / Batch (4/17) / Loss 7.461574\n",
      "Epoch 11 / Batch (5/17) / Loss 7.412116\n",
      "Epoch 11 / Batch (6/17) / Loss 6.666286\n",
      "Epoch 11 / Batch (7/17) / Loss 6.039383\n",
      "Epoch 11 / Batch (8/17) / Loss 5.995492\n",
      "Epoch 11 / Batch (9/17) / Loss 5.097156\n",
      "Epoch 11 / Batch (10/17) / Loss 5.411010\n",
      "Epoch 11 / Batch (11/17) / Loss 4.876823\n",
      "Epoch 11 / Batch (12/17) / Loss 4.504481\n",
      "Epoch 11 / Batch (13/17) / Loss 4.456665\n",
      "Epoch 11 / Batch (14/17) / Loss 3.798184\n",
      "Epoch 11 / Batch (15/17) / Loss 3.620638\n",
      "Epoch 11 / Batch (16/17) / Loss 3.815871\n",
      "Epoch 11 / Batch (17/17) / Loss 3.285494\n",
      "Epoch 11 complete, 7.242825984954834 seconds elapsed\n",
      "Epoch 12 starting..\n",
      "Epoch 12 complete, 7.175312280654907 seconds elapsed\n",
      "Epoch 13 starting..\n",
      "Epoch 13 complete, 7.211984634399414 seconds elapsed\n",
      "Epoch 14 starting..\n",
      "Epoch 14 complete, 7.192951202392578 seconds elapsed\n",
      "Epoch 15 starting..\n",
      "Epoch 15 complete, 7.227720022201538 seconds elapsed\n",
      "Epoch 16 starting..\n",
      "Epoch 16 complete, 7.449299335479736 seconds elapsed\n",
      "Epoch 17 starting..\n",
      "Epoch 17 complete, 7.196493625640869 seconds elapsed\n",
      "Epoch 18 starting..\n",
      "Epoch 18 complete, 7.311708688735962 seconds elapsed\n",
      "Epoch 19 starting..\n",
      "Epoch 19 complete, 7.309297800064087 seconds elapsed\n",
      "Epoch 20 starting..\n",
      "Epoch 20 complete, 7.27350378036499 seconds elapsed\n",
      "Epoch 21 starting..\n",
      "Epoch 21 / Batch (1/17) / Loss 1.737404\n",
      "Epoch 21 / Batch (2/17) / Loss 1.716984\n",
      "Epoch 21 / Batch (3/17) / Loss 1.722495\n",
      "Epoch 21 / Batch (4/17) / Loss 1.738412\n",
      "Epoch 21 / Batch (5/17) / Loss 1.723538\n",
      "Epoch 21 / Batch (6/17) / Loss 1.722214\n",
      "Epoch 21 / Batch (7/17) / Loss 1.735219\n",
      "Epoch 21 / Batch (8/17) / Loss 1.723820\n",
      "Epoch 21 / Batch (9/17) / Loss 1.722567\n",
      "Epoch 21 / Batch (10/17) / Loss 1.708747\n",
      "Epoch 21 / Batch (11/17) / Loss 1.723457\n",
      "Epoch 21 / Batch (12/17) / Loss 1.728440\n",
      "Epoch 21 / Batch (13/17) / Loss 1.721109\n",
      "Epoch 21 / Batch (14/17) / Loss 1.714699\n",
      "Epoch 21 / Batch (15/17) / Loss 1.712918\n",
      "Epoch 21 / Batch (16/17) / Loss 1.743325\n",
      "Epoch 21 / Batch (17/17) / Loss 1.724698\n",
      "Epoch 21 complete, 7.231215953826904 seconds elapsed\n",
      "Epoch 22 starting..\n",
      "Epoch 22 complete, 7.086533546447754 seconds elapsed\n",
      "Epoch 23 starting..\n",
      "Epoch 23 complete, 7.106343030929565 seconds elapsed\n",
      "Epoch 24 starting..\n",
      "Epoch 24 complete, 7.11220121383667 seconds elapsed\n",
      "Epoch 25 starting..\n",
      "Epoch 25 complete, 7.099947452545166 seconds elapsed\n",
      "Epoch 26 starting..\n",
      "Epoch 26 complete, 7.197391986846924 seconds elapsed\n",
      "Epoch 27 starting..\n",
      "Epoch 27 complete, 7.115341901779175 seconds elapsed\n",
      "Epoch 28 starting..\n",
      "Epoch 28 complete, 7.183591604232788 seconds elapsed\n",
      "Epoch 29 starting..\n",
      "Epoch 29 complete, 7.111870765686035 seconds elapsed\n",
      "Epoch 30 starting..\n",
      "Epoch 30 complete, 7.200535774230957 seconds elapsed\n",
      "Epoch 31 starting..\n",
      "Epoch 31 / Batch (1/17) / Loss 1.679991\n",
      "Epoch 31 / Batch (2/17) / Loss 1.704007\n",
      "Epoch 31 / Batch (3/17) / Loss 1.679714\n",
      "Epoch 31 / Batch (4/17) / Loss 1.726849\n",
      "Epoch 31 / Batch (5/17) / Loss 1.687507\n",
      "Epoch 31 / Batch (6/17) / Loss 1.705065\n",
      "Epoch 31 / Batch (7/17) / Loss 1.705273\n",
      "Epoch 31 / Batch (8/17) / Loss 1.702732\n",
      "Epoch 31 / Batch (9/17) / Loss 1.698298\n",
      "Epoch 31 / Batch (10/17) / Loss 1.705545\n",
      "Epoch 31 / Batch (11/17) / Loss 1.696027\n",
      "Epoch 31 / Batch (12/17) / Loss 1.719238\n",
      "Epoch 31 / Batch (13/17) / Loss 1.696192\n",
      "Epoch 31 / Batch (14/17) / Loss 1.700254\n",
      "Epoch 31 / Batch (15/17) / Loss 1.718643\n",
      "Epoch 31 / Batch (16/17) / Loss 1.691639\n",
      "Epoch 31 / Batch (17/17) / Loss 1.716774\n",
      "Epoch 31 complete, 7.243197679519653 seconds elapsed\n",
      "Epoch 32 starting..\n",
      "Epoch 32 complete, 7.2499425411224365 seconds elapsed\n",
      "Epoch 33 starting..\n",
      "Epoch 33 complete, 7.353803873062134 seconds elapsed\n",
      "Epoch 34 starting..\n",
      "Epoch 34 complete, 7.184979677200317 seconds elapsed\n",
      "Epoch 35 starting..\n",
      "Epoch 35 complete, 7.14730978012085 seconds elapsed\n",
      "Epoch 36 starting..\n",
      "Epoch 36 complete, 7.1839210987091064 seconds elapsed\n",
      "Epoch 37 starting..\n",
      "Epoch 37 complete, 7.236520767211914 seconds elapsed\n",
      "Epoch 38 starting..\n",
      "Epoch 38 complete, 7.227398157119751 seconds elapsed\n",
      "Epoch 39 starting..\n",
      "Epoch 39 complete, 7.347156763076782 seconds elapsed\n",
      "Epoch 40 starting..\n",
      "Epoch 40 complete, 7.404252290725708 seconds elapsed\n",
      "Epoch 41 starting..\n",
      "Epoch 41 / Batch (1/17) / Loss 1.669725\n",
      "Epoch 41 / Batch (2/17) / Loss 1.665758\n",
      "Epoch 41 / Batch (3/17) / Loss 1.684826\n",
      "Epoch 41 / Batch (4/17) / Loss 1.674345\n",
      "Epoch 41 / Batch (5/17) / Loss 1.683996\n",
      "Epoch 41 / Batch (6/17) / Loss 1.668077\n",
      "Epoch 41 / Batch (7/17) / Loss 1.670302\n",
      "Epoch 41 / Batch (8/17) / Loss 1.648536\n",
      "Epoch 41 / Batch (9/17) / Loss 1.677299\n",
      "Epoch 41 / Batch (10/17) / Loss 1.650087\n",
      "Epoch 41 / Batch (11/17) / Loss 1.675259\n",
      "Epoch 41 / Batch (12/17) / Loss 1.664847\n",
      "Epoch 41 / Batch (13/17) / Loss 1.676866\n",
      "Epoch 41 / Batch (14/17) / Loss 1.688153\n",
      "Epoch 41 / Batch (15/17) / Loss 1.672871\n",
      "Epoch 41 / Batch (16/17) / Loss 1.660657\n",
      "Epoch 41 / Batch (17/17) / Loss 1.661113\n",
      "Epoch 41 complete, 7.400242567062378 seconds elapsed\n",
      "Epoch 42 starting..\n",
      "Epoch 42 complete, 7.318349361419678 seconds elapsed\n",
      "Epoch 43 starting..\n",
      "Epoch 43 complete, 7.303819179534912 seconds elapsed\n",
      "Epoch 44 starting..\n",
      "Epoch 44 complete, 7.345340251922607 seconds elapsed\n",
      "Epoch 45 starting..\n",
      "Epoch 45 complete, 7.344338893890381 seconds elapsed\n",
      "Epoch 46 starting..\n",
      "Epoch 46 complete, 7.27629280090332 seconds elapsed\n",
      "Epoch 47 starting..\n",
      "Epoch 47 complete, 7.241521120071411 seconds elapsed\n",
      "Epoch 48 starting..\n",
      "Epoch 48 complete, 7.333338260650635 seconds elapsed\n",
      "Epoch 49 starting..\n",
      "Epoch 49 complete, 7.47764778137207 seconds elapsed\n",
      "Epoch 50 starting..\n",
      "Epoch 50 complete, 7.39329981803894 seconds elapsed\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "loss_values = []\n",
    "for epoch in range(n_epochs):\n",
    "    print('Epoch {} starting..'.format(epoch+1))\n",
    "    epoch_start = time.time()\n",
    "    classes_epoch, imgs_epoch = ut.shuffle_xy(classes_train,imgs_train) \n",
    "    n_batches = len(classes_epoch)//(n_classes*batch_size)\n",
    "    for batch in range(n_batches):\n",
    "        classes_batch = classes_epoch[batch*n_classes*batch_size:(batch+1)*n_classes*batch_size]\n",
    "        imgs_batch = imgs_epoch[batch*n_classes*batch_size:(batch+1)*n_classes*batch_size]\n",
    "\n",
    "        Xl_batch, yl_batch = [], []\n",
    "        for episode in range(batch_size):\n",
    "            imgs_ep = imgs_batch[episode*n_classes:(episode+1)*n_classes]\n",
    "            Xl_ep, yl_ep = [], []\n",
    "            for ind, cat in enumerate(imgs_ep):\n",
    "                for arr in cat:\n",
    "                    Xl_ep.append(arr)\n",
    "                    yl_ep.append(ut.one_hot(ind,n_classes))\n",
    "            Xl_shuff, yl_shuff = ut.shuffle_xy(Xl_ep,yl_ep)\n",
    "            X_arr, y_arr = np.asarray(Xl_shuff), np.asarray(yl_shuff)\n",
    "            Xl_batch.append(X_arr)\n",
    "            yl_batch.append(y_arr)\n",
    "        X_train, y_train = np.asarray(Xl_batch), np.asarray(yl_batch)\n",
    "        X_mann, y_mann = get_graph_input(X_train, y_train)\n",
    "        optimizer.zero_grad()\n",
    "        M, h, c, wu, wr, r, o = net(X_mann)\n",
    "        outs = o.view(100*16, 5)\n",
    "        gt = y_mann.view(100*16)\n",
    "        loss = criterion(outs, gt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_d = loss.item()\n",
    "        _, predicted = torch.max(outs.data, 1)\n",
    "        correct = (predicted == gt).sum().item()\n",
    "        loss_values.append(loss_d)\n",
    "        if(epoch%10==0):\n",
    "            print('Epoch %d'%(epoch+1),'/ Batch (%d/%d)'%(batch+1,n_batches),'/ Loss %f'%(loss_d))\n",
    "    epoch_end = time.time()\n",
    "    time_elapsed = epoch_end-epoch_start\n",
    "    print('Epoch {} complete,'.format(epoch+1),time_elapsed,'seconds elapsed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_loss(train,name1=\"train_loss\"):\n",
    "    plt.plot(train, label=name1)\n",
    "    plt.legend()\n",
    "\n",
    "plot_loss(loss_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check performance on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = len(classes_val) // n_classes\n",
    "Xl_batch, yl_batch = [], []\n",
    "for episode in range(batch_size):\n",
    "    imgs_ep = imgs_val[episode*n_classes:(episode+1)*n_classes]\n",
    "    Xl_ep, yl_ep = [], []\n",
    "    for ind, cat in enumerate(imgs_ep):\n",
    "        for arr in cat:\n",
    "            Xl_ep.append(arr)\n",
    "            yl_ep.append(ut.one_hot(ind,n_classes))\n",
    "    Xl_shuff, yl_shuff = ut.shuffle_xy(Xl_ep,yl_ep)\n",
    "    X_arr, y_arr = np.asarray(Xl_shuff), np.asarray(yl_shuff)\n",
    "    Xl_batch.append(X_arr)\n",
    "    yl_batch.append(y_arr)\n",
    "X_val, y_val = np.asarray(Xl_batch), np.asarray(yl_batch)\n",
    "X_mann, y_mann = get_graph_input(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-4450f92e46be>:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  wr_t = F.softmax(inner_prod/norm_prod)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "802\n"
     ]
    }
   ],
   "source": [
    "M, h, c, wu, wr, r, o = net(X_mann, batch_size=batch_size)\n",
    "outs = o.view(100*batch_size, 5)\n",
    "gt = y_mann.view(100*batch_size)\n",
    "_, predicted = torch.max(outs.data, 1)\n",
    "correct = (predicted == gt).sum().item()\n",
    "print(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2005\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {correct/len(gt)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is exactly the same as randomly choosing a class! Something is wrong :("
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
