{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Attentive Meta-Learner (SNAIL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Allow GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Fix multiprocessing warning from num_workers in torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore InterpolationMode warnings from torchmeta\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import math\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchmeta.datasets.helpers import omniglot\n",
    "from torchmeta.utils.data import BatchMetaDataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### `blocks.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CasualConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, \n",
    "                 stride=1, dilation=1, groups=1, bias=True):\n",
    "        super(CasualConv1d, self).__init__()\n",
    "        self.dilation = dilation\n",
    "        padding = dilation * (kernel_size - 1)\n",
    "        self.conv1d = nn.Conv1d(in_channels, out_channels, kernel_size, stride,\n",
    "                                padding, dilation, groups, bias)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Takes something of shape (N, in_channels, T),\n",
    "        # returns (N, out_channels, T)\n",
    "        out = self.conv1d(input)\n",
    "        return out[:, :, :-self.dilation] # TODO: make this correct for different strides/padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, in_channels, dilation, filters, kernel_size=2):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        self.casualconv1 = CasualConv1d(in_channels, filters, kernel_size, dilation=dilation)\n",
    "        self.casualconv2 = CasualConv1d(in_channels, filters, kernel_size, dilation=dilation)\n",
    "\n",
    "    def forward(self, input_):\n",
    "        # input is dimensions (N, in_channels, T)\n",
    "        xf = self.casualconv1(input_)\n",
    "        xg = self.casualconv2(input_)\n",
    "        activations = torch.tanh(xf) * torch.sigmoid(xg) # shape: (N, filters, T)\n",
    "\n",
    "        return torch.cat((input_, activations), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCBlock(nn.Module):\n",
    "    def __init__(self, in_channels, seq_length, filters):\n",
    "        super(TCBlock, self).__init__()\n",
    "        self.dense_blocks = nn.ModuleList([DenseBlock(in_channels + i * filters, 2 ** (i+1), filters)\n",
    "                                           for i in range(int(math.ceil(math.log(seq_length, 2))))])\n",
    "\n",
    "    def forward(self, input):\n",
    "        # input is dimensions (N, T, in_channels)\n",
    "        input = torch.transpose(input, 1, 2)\n",
    "        for block in self.dense_blocks:\n",
    "            input = block(input)\n",
    "        return torch.transpose(input, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, key_size, value_size):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.linear_query = nn.Linear(in_channels, key_size)\n",
    "        self.linear_keys = nn.Linear(in_channels, key_size)\n",
    "        self.linear_values = nn.Linear(in_channels, value_size)\n",
    "        self.sqrt_key_size = math.sqrt(key_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # input is dim (N, T, in_channels) where N is the batch_size, and T is\n",
    "        # the sequence length\n",
    "        mask = np.array([[1 if i>j else 0 for i in range(input.shape[1])] for j in range(input.shape[1])])\n",
    "#         mask = torch.BoolTensor(mask).cuda()\n",
    "        mask = torch.BoolTensor(mask)\n",
    "\n",
    "        #import pdb; pdb.set_trace()\n",
    "        keys = self.linear_keys(input) # shape: (N, T, key_size)\n",
    "        query = self.linear_query(input) # shape: (N, T, key_size)\n",
    "        values = self.linear_values(input) # shape: (N, T, value_size)\n",
    "        temp = torch.bmm(query, torch.transpose(keys, 1, 2)) # shape: (N, T, T)\n",
    "        temp.data.masked_fill_(mask, -float('inf'))\n",
    "        temp = F.softmax(temp / self.sqrt_key_size, dim=1) # shape: (N, T, T), broadcasting over any slice [:, x, :], each row of the matrix\n",
    "        temp = torch.bmm(temp, values) # shape: (N, T, value_size)\n",
    "        return torch.cat((input, temp), dim=2) # shape: (N, T, in_channels + value_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### `resnet_blocks.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(in_planes, out_planes, kernel_size=3, stride=1, padding=1):\n",
    "    \"\"\"convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n",
    "                     padding=padding, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(in_channels, out_channels):\n",
    "    '''\n",
    "    returns a block conv-bn-relu-pool\n",
    "    '''\n",
    "    return nn.Sequential(OrderedDict([\n",
    "        ('conv', nn.Conv2d(in_channels, out_channels, 3, padding=1)),\n",
    "        ('bn', nn.BatchNorm2d(out_channels, momentum=1)),\n",
    "        #('bn', nn.BatchNorm2d(out_channels)),\n",
    "        ('relu', nn.ReLU()),\n",
    "        ('pool', nn.MaxPool2d(2))\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchnorm(input, weight=None, bias=None, running_mean=None, running_var=None, training=True,eps=1e-5, momentum=0.1):\n",
    "    # momentum = 1 restricts stats to the current mini-batch\n",
    "    # This hack only works when momentum is 1 and avoids needing to track\n",
    "    # running stats by substituting dummy variables\n",
    "    size = int(np.prod(np.array(input.data.size()[1])))\n",
    "#     running_mean = torch.zeros(size).cuda()\n",
    "#     running_var = torch.ones(size).cuda()\n",
    "    running_mean = torch.zeros(size)\n",
    "    running_var = torch.ones(size)\n",
    "    return F.batch_norm(input, running_mean, running_var, weight, bias, training, momentum, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OmniglotNet(nn.Module):\n",
    "    '''\n",
    "    Model as described in the reference paper,\n",
    "    source: https://github.com/jakesnell/prototypical-networks/blob/f0c48808e496989d01db59f86d4449d7aee9ab0c/protonets/models/few_shot.py#L62-L84\n",
    "    '''\n",
    "    def __init__(self, x_dim=1, hid_dim=64, z_dim=64):\n",
    "        super(OmniglotNet, self).__init__()\n",
    "        self.encoder = nn.Sequential(OrderedDict([\n",
    "            ('block1', conv_block(x_dim, hid_dim)),\n",
    "            ('block2', conv_block(hid_dim, hid_dim)),\n",
    "            ('block3', conv_block(hid_dim, hid_dim)),\n",
    "            ('block4', conv_block(hid_dim, z_dim)),\n",
    "        ]))\n",
    "\n",
    "    def forward(self, x, weights=None):\n",
    "        if weights is None:\n",
    "            x = self.encoder(x)\n",
    "        else:\n",
    "            x = F.conv2d(x, weights['encoder.block1.conv.weight'], weights['encoder.block1.conv.bias'])\n",
    "            x = batchnorm(x, weight=weights['encoder.block1.bn.weight'], bias=weights['encoder.block1.bn.bias'])\n",
    "            x = F.relu(x)\n",
    "            x = F.max_pool2d(x, 2, 2)\n",
    "            x = F.conv2d(x, weights['encoder.block2.conv.weight'], weights['encoder.block2.conv.bias'])\n",
    "            x = batchnorm(x, weight=weights['encoder.block2.bn.weight'], bias=weights['encoder.block2.bn.bias'])\n",
    "            x = F.relu(x)\n",
    "            x = F.max_pool2d(x, 2, 2)\n",
    "            x = F.conv2d(x, weights['encoder.block3.conv.weight'], weights['encoder.block3.conv.bias'])\n",
    "            x = batchnorm(x, weight=weights['encoder.block3.bn.weight'], bias=weights['encoder.block3.bn.bias'])\n",
    "            x = F.relu(x)\n",
    "            x = F.max_pool2d(x, 2, 2)\n",
    "            x = F.conv2d(x, weights['encoder.block4.conv.weight'], weights['encoder.block4.conv.bias'])\n",
    "            x = batchnorm(x, weight=weights['encoder.block4.bn.weight'], bias=weights['encoder.block4.bn.bias'])\n",
    "            x = F.relu(x)\n",
    "            x = F.max_pool2d(x, 2, 2)\n",
    "        return x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, filters, pool_padding=0):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = conv(in_channels, filters)\n",
    "        self.bn1 = nn.BatchNorm2d(filters)\n",
    "        self.relu1 = nn.LeakyReLU()\n",
    "        self.conv2 = conv(filters, filters)\n",
    "        self.bn2 = nn.BatchNorm2d(filters)\n",
    "        self.relu2 = nn.LeakyReLU()\n",
    "        self.conv3 = conv(filters, filters)\n",
    "        self.bn3 = nn.BatchNorm2d(filters)\n",
    "        self.relu3 = nn.LeakyReLU()\n",
    "        self.conv4 = conv(in_channels, filters, kernel_size=1, padding=0)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(2, padding=pool_padding)\n",
    "        self.dropout = nn.Dropout(p=0.9)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.conv4(x)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu2(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.relu3(out)\n",
    "\n",
    "        out += residual\n",
    "        out = self.maxpool(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### `snail.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnailFewShot(nn.Module):\n",
    "    def __init__(self, N, K):\n",
    "        # N-way, K-shot\n",
    "        super(SnailFewShot, self).__init__()\n",
    "        self.encoder = OmniglotNet()\n",
    "        num_channels = 64 + N\n",
    "        num_filters = int(math.ceil(math.log(N * K + 1, 2)))\n",
    "        self.attention1 = AttentionBlock(num_channels, 64, 32)\n",
    "        num_channels += 32\n",
    "        self.tc1 = TCBlock(num_channels, N * K + 1, 128)\n",
    "        num_channels += num_filters * 128\n",
    "        self.attention2 = AttentionBlock(num_channels, 256, 128)\n",
    "        num_channels += 128\n",
    "        self.tc2 = TCBlock(num_channels, N * K + 1, 128)\n",
    "        num_channels += num_filters * 128\n",
    "        self.attention3 = AttentionBlock(num_channels, 512, 256)\n",
    "        num_channels += 256\n",
    "        self.fc = nn.Linear(num_channels, N)\n",
    "        self.N = N\n",
    "        self.K = K\n",
    "\n",
    "    def forward(self, input, labels):\n",
    "        x = self.encoder(input)\n",
    "        batch_size = int(labels.size()[0] / (self.N * self.K + 1))\n",
    "\n",
    "        # TODO: Maybe move this zeroing to process_torchmeta_batch() at train.py\n",
    "        last_idxs = [(i + 1) * (self.N * self.K + 1) - 1 for i in range(batch_size)]\n",
    "#         labels[last_idxs] = torch.Tensor(np.zeros((batch_size, labels.size()[1]))).cuda()\n",
    "        labels[last_idxs] = torch.Tensor(np.zeros((batch_size, labels.size()[1])))\n",
    "\n",
    "        x = torch.cat((x, labels), 1)\n",
    "        x = x.view((batch_size, self.N * self.K + 1, -1))\n",
    "        x = self.attention1(x)\n",
    "        x = self.tc1(x)\n",
    "        x = self.attention2(x)\n",
    "        x = self.tc2(x)\n",
    "        x = self.attention3(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(last_model, last_targets):\n",
    "    _, preds = last_model.max(1)\n",
    "    acc = torch.eq(preds, last_targets).float().mean()\n",
    "    return acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Update docstring\n",
    "def process_torchmeta_batch(batch, NUM_CLS, NUM_SAMPLES):\n",
    "    \"\"\"\n",
    "    Process batch from torchmeta dataset for the SNAIL model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch : dict\n",
    "        A dictionary given by the torchmeta dataset\n",
    "    options : SimpleNamespace\n",
    "        A namespace with configuration details.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    input_images : torch.tensor\n",
    "        Input images to the SNAIL model of shape (batch_size*(N*K+1), img_channels, img_height,\n",
    "        img_width) where N and K denote the same variables as in the N-way K-shot problem.\n",
    "    input_onehot_labels : torch.tensor\n",
    "        Input one-hot labels to the SNAIL model of shape (batch_size*(N*K+1), N) where N and K\n",
    "        denote the same variables as in the N-way K-shot problem. The last label for each (N*K+1)\n",
    "        is not used since it is the target label.\n",
    "    target_labels : torch.tensor\n",
    "        Test set labels to evaluate the SNAIL model by comparing with its outputs. Has shape\n",
    "        (batch_size).\n",
    "\n",
    "    \"\"\"\n",
    "    train_inputs, train_labels = batch[\"train\"]\n",
    "    test_inputs, test_labels = batch[\"test\"]\n",
    "\n",
    "    # Select one image from N images in the test set\n",
    "    chosen_indices = torch.randint(test_inputs.shape[1], size=(test_inputs.shape[0],))\n",
    "    chosen_test_inputs = test_inputs[torch.arange(test_inputs.shape[0]), chosen_indices, :, :, :].unsqueeze(1)\n",
    "    chosen_test_labels = test_labels[torch.arange(test_labels.shape[0]), chosen_indices].unsqueeze(1)\n",
    "\n",
    "    # Concatenate train and test set for SNAIL-style input images and labels\n",
    "    input_images = torch.cat((train_inputs, chosen_test_inputs), dim=1).reshape((-1, *train_inputs.shape[2:]))\n",
    "    input_labels = torch.cat((train_labels, chosen_test_labels), dim=1).reshape((-1, *train_labels.shape[2:]))\n",
    "\n",
    "    # Convert labels to one-hot\n",
    "    input_onehot_labels = F.one_hot(input_labels).float()\n",
    "\n",
    "    # Separate out target labels\n",
    "    target_labels = input_labels[::(NUM_CLS * NUM_SAMPLES + 1)].long()\n",
    "\n",
    "    # Move to correct device\n",
    "#     if options.cuda:\n",
    "#         input_images, input_onehot_labels = input_images.cuda(), input_onehot_labels.cuda()\n",
    "#         target_labels = target_labels.cuda()\n",
    "\n",
    "    return input_images, input_onehot_labels, target_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRAIN_EPISODES = 100\n",
    "N_VAL_EPISODES = 100\n",
    "N_TEST_EPISODES = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"default\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADAM_LR = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 32\n",
    "VAL_BATCH_SIZE = 32\n",
    "TEST_BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLS = 5\n",
    "NUM_SAMPLES = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available() and not options.cuda:\n",
    "    print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main script for training SNAIL on Omniglot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_episode(model, optimizer, batch, NUM_CLS, NUM_SAMPLES):\n",
    "    input_images, input_onehot_labels, target_labels = process_torchmeta_batch(batch, NUM_CLS, NUM_SAMPLES)\n",
    "    predicted_labels = model(input_images, input_onehot_labels)[:, -1, :]\n",
    "    loss = F.cross_entropy(predicted_labels, target_labels)\n",
    "    acc = get_acc(predicted_labels, target_labels)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item(), acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_one_episode(model, optimizer, batch, NUM_CLS, NUM_SAMPLES):\n",
    "    input_images, input_onehot_labels, target_labels = process_torchmeta_batch(batch, NUM_CLS, NUM_SAMPLES)\n",
    "    predicted_labels = model(input_images, input_onehot_labels)[:, -1, :]\n",
    "    loss = F.cross_entropy(predicted_labels, target_labels)\n",
    "    acc = get_acc(predicted_labels, target_labels)\n",
    "\n",
    "    return loss.item(), acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_dataloader, val_dataloader):\n",
    "    if val_dataloader is None:\n",
    "        best_state = None\n",
    "    train_loss_per_ep, train_acc_per_ep = [], []\n",
    "    val_loss_per_ep, val_acc_per_ep = [], []\n",
    "    best_val_acc = 0\n",
    "\n",
    "    best_model_path = os.path.join(OUTPUT_DIR, 'best_model.pth')\n",
    "    last_model_path = os.path.join(OUTPUT_DIR, 'last_model.pth')\n",
    "\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    for i, batch in tqdm(enumerate(train_dataloader), total=N_TRAIN_EPISODES):\n",
    "        if i >= N_TRAIN_EPISODES: break\n",
    "\n",
    "        train_loss, train_acc = train_one_episode(model, optimizer, batch, NUM_CLS, NUM_SAMPLES)\n",
    "        train_loss_per_ep.append(train_loss)\n",
    "        train_acc_per_ep.append(train_acc)\n",
    "\n",
    "    # TODO: Don't average all episodes here\n",
    "    avg_train_loss = np.mean(train_loss)\n",
    "    avg_train_acc = np.mean(train_acc)\n",
    "    print('Avg Train Loss: {}, Avg Train Acc: {}'.format(avg_train_loss, avg_train_acc))\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    for i, batch in tqdm(enumerate(val_dataloader), total=N_VAL_EPISODES):\n",
    "        if i >= N_VAL_EPISODES: break\n",
    "\n",
    "        val_loss, val_acc = test_one_episode(model, optimizer, batch, NUM_CLS, NUM_SAMPLES)\n",
    "        val_loss_per_ep.append(val_loss)\n",
    "        val_acc_per_ep.append(val_acc)\n",
    "\n",
    "    avg_val_loss = np.mean(val_loss)\n",
    "    avg_val_acc = np.mean(val_acc)\n",
    "    postfix = ' (Best)' if avg_val_acc >= best_val_acc else ' (Best: {})'.format(best_val_acc)\n",
    "    print('Avg Val Loss: {}, Avg Val Acc: {}{}'.format(avg_val_loss, avg_val_acc, postfix))\n",
    "\n",
    "    if avg_val_acc >= best_val_acc:\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        best_val_acc = avg_val_acc\n",
    "        best_state = model.state_dict()\n",
    "\n",
    "    for name in ['train_loss_per_ep', 'train_acc_per_ep', 'val_loss_per_ep', 'val_acc_per_ep']:\n",
    "        with open(os.path.join(OUTPUT_DIR, name + '.txt'), 'w') as f:\n",
    "            for item in locals()[name]:\n",
    "                f.write(\"%s\\n\" % item)\n",
    "\n",
    "    torch.save(model.state_dict(), last_model_path)\n",
    "\n",
    "    return best_state, best_val_acc, train_loss_per_ep, train_acc_per_ep, val_loss_per_ep, val_acc_per_ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_dataloader):\n",
    "    \"\"\"\n",
    "    Test model on given dataset and options.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_loss_per_ep, test_acc_per_ep = [], []\n",
    "    for i, batch in tqdm(enumerate(test_dataloader), total=N_TEST_EPISODES):\n",
    "        if i >= N_TEST_EPISODES: break\n",
    "\n",
    "        test_loss, test_acc = test_one_episode(model, optimizer, batch, NUM_CLS, NUM_SAMPLES)\n",
    "        test_loss_per_ep.append(val_loss)\n",
    "        test_acc_per_ep.append(val_acc)\n",
    "\n",
    "    avg_test_loss = np.mean(test_loss_per_ep)\n",
    "    avg_test_acc = np.mean(test_acc_per_ep)\n",
    "    print('Avg Test Loss: {} Avg Test Acc: {}'.format(avg_test_loss, avg_test_acc))\n",
    "\n",
    "    return avg_test_loss, avg_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup dataset\n",
    "train_dataset = omniglot(\"data\", ways=5, shots=1, test_shots=1, meta_train=True, download=True)\n",
    "train_dataloader = BatchMetaDataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, num_workers=2)\n",
    "val_dataset = omniglot(\"data\", ways=5, shots=1, test_shots=1, meta_val=True, download=True)\n",
    "val_dataloader = BatchMetaDataLoader(val_dataset, batch_size=VAL_BATCH_SIZE, num_workers=2)\n",
    "test_dataset = omniglot(\"data\", ways=5, shots=1, test_shots=1, meta_test=True, download=True)\n",
    "test_dataloader = BatchMetaDataLoader(test_dataset, batch_size=TEST_BATCH_SIZE, num_workers=2)\n",
    "# Setup model\n",
    "model = SnailFewShot(NUM_CLS, NUM_SAMPLES)\n",
    "# model = model.cuda() if options.cuda else model\n",
    "# Setup optimizer\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=ADAM_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 100/100 [00:56<00:00,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Train Loss: 1.5750852823257446, Avg Train Acc: 0.3125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                      | 0/100 [00:00<?, ?it/s]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc77da82790>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1324, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1316, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc77da82790>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1324, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1316, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "100%|████████████████████████████████████████████| 100/100 [00:21<00:00,  4.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Val Loss: 1.6460012197494507, Avg Val Acc: 0.15625 (Best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "train_result = train(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    ")\n",
    "best_state, best_acc, train_loss, train_acc, val_loss, val_acc = train_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with last model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                      | 0/100 [00:00<?, ?it/s]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc77da82790>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1324, in __del__\n",
      "Exception ignored in:     self._shutdown_workers()<function _MultiProcessingDataLoaderIter.__del__ at 0x7fc77da82790>\n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1316, in _shutdown_workers\n",
      "\n",
      "    if w.is_alive():Traceback (most recent call last):\n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1324, in __del__\n",
      "\n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "    AssertionErrorself._shutdown_workers(): can only test a child process\n",
      "\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc77da82790>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1324, in __del__\n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1316, in _shutdown_workers\n",
      "    if w.is_alive():    \n",
      "self._shutdown_workers()  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    \n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1316, in _shutdown_workers\n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "    if w.is_alive():AssertionError\n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: : can only test a child processcan only test a child process\n",
      "\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc77da82790>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1324, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1316, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "100%|████████████████████████████████████████████| 100/100 [00:20<00:00,  4.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Test Loss: 1.620465074777603 Avg Test Acc: 0.1946875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.620465074777603, 0.1946875)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test last model\n",
    "test(\n",
    "    model=model,\n",
    "    test_dataloader=test_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with best model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                      | 0/100 [00:00<?, ?it/s]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc77da82790>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1324, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1316, in _shutdown_workers\n",
      "    Exception ignored in: if w.is_alive():\n",
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x7fc77da82790>  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "\n",
      "Traceback (most recent call last):\n",
      "Exception ignored in:   File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1324, in __del__\n",
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x7fc77da82790>    \n",
      "self._shutdown_workers()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1316, in _shutdown_workers\n",
      "      File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1324, in __del__\n",
      "if w.is_alive():\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1316, in _shutdown_workers\n",
      "    if w.is_alive():  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "\n",
      "      File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'    \n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionErrorAssertionError: : can only test a child processcan only test a child process\n",
      "\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc77da82790>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1324, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1316, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/home/seungjaeryanlee/anaconda3/envs/meta-learning-book/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "100%|████████████████████████████████████████████| 100/100 [00:20<00:00,  4.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Test Loss: 1.620465074777603 Avg Test Acc: 0.1946875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.620465074777603, 0.1946875)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test best model\n",
    "model.load_state_dict(best_state)\n",
    "test(\n",
    "    test_dataloader=test_dataloader,\n",
    "    model=model,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:meta-learning-book]",
   "language": "python",
   "name": "conda-env-meta-learning-book-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
