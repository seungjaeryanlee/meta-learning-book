{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Augmented Neural Network using Omniglot Dataset\n",
    "\n",
    "\n",
    "In this tutorial, we will do following things step by step:\n",
    "1. Data Preprocessing: Creating Pairs.\n",
    "2. Create a Memory Augmented Enural Network\n",
    "3. Train it using Omniglot dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo apt-get install python3-opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /home/seungjaeryanlee/anaconda3/lib/python3.8/site-packages (4.5.3.56)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/seungjaeryanlee/anaconda3/lib/python3.8/site-packages (from opencv-python) (1.19.5)\n",
      "Requirement already satisfied: Pillow in /home/seungjaeryanlee/anaconda3/lib/python3.8/site-packages (8.3.1)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Move to requirements.txt\n",
    "# !pip install scipy==1.1.0\n",
    "!pip install opencv-python\n",
    "!pip install Pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step1: Lets first import all libraries needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils as ut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step2 : Load Data\n",
    "\n",
    "We are reading images from two folders named 'image_background', 'image_evaluation' defined in 'data' directory\n",
    "\n",
    "Dataset is divided into 1423 charcters images for training and rest for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1623 total character classes\n",
      "1423 characters assigned for training, 200 characters assigned for validation\n"
     ]
    }
   ],
   "source": [
    "width = 20\n",
    "\n",
    "# gather data paths\n",
    "subfolds = ut.extend_children('data','')\n",
    "datafolds = [subfolds[0],subfolds[1]]\n",
    "alphabets = ut.extend_generation(datafolds,'')\n",
    "charpaths = ut.extend_generation(alphabets,'')\n",
    "chars_dataset = [v.split('/')[2]+'/'+v.split('/')[3] for v in charpaths]\n",
    "\n",
    "# index-value conversion dictionaries for character set\n",
    "i2v = {i:v for i, v in enumerate(chars_dataset)}\n",
    "v2i = {v:i for i, v in enumerate(chars_dataset)}\n",
    "\n",
    "# get size of dataset\n",
    "mc_dataset = len(charpaths)\n",
    "print(mc_dataset,'total character classes')\n",
    "\n",
    "# train split\n",
    "mc_train = 1423\n",
    "chars_train = chars_dataset[:mc_train]\n",
    "classes_train = [v2i[v] for v in chars_train]\n",
    "\n",
    "# validation split\n",
    "mc_val = mc_dataset-mc_train\n",
    "chars_val = chars_dataset[-mc_val:]\n",
    "classes_val = [v2i[v] for v in chars_val]\n",
    "\n",
    "\n",
    "\n",
    "print('%s characters assigned for training, %s characters assigned for validation'%(mc_train,mc_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Read Data\n",
    "\n",
    "All character images are read and saved in the imgs_dataset variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1623 character folders loaded\n",
      "200/1623 character folders loaded\n",
      "400/1623 character folders loaded\n",
      "600/1623 character folders loaded\n",
      "800/1623 character folders loaded\n",
      "1000/1623 character folders loaded\n",
      "1200/1623 character folders loaded\n",
      "1400/1623 character folders loaded\n",
      "1600/1623 character folders loaded\n"
     ]
    }
   ],
   "source": [
    "# load images from character paths\n",
    "imgs_dataset = []\n",
    "for i, charfold in enumerate(charpaths):\n",
    "    if i%200==0:\n",
    "        print('%s/%s character folders loaded'%(i,mc_dataset))\n",
    "    imgs_dataset.append([ ut.load_image(imgpath,(width,width))/127.5-1 for imgpath in ut.extend_children(charfold,'.png') ] )\n",
    "# access imgs_dataset [ character index ] [ sample index ] [ row,col ]\n",
    "\n",
    "# split images between train and validation sets\n",
    "imgs_train = imgs_dataset[:mc_train]\n",
    "imgs_val = imgs_dataset[-mc_val:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Let's Initialize our Hyper-paramters\n",
    "\n",
    "We define all the hyper-parameters and dimensions of all the variables here.\n",
    "* n_classes is 5 (in each batch we have 5 types of characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_classes = 5\n",
    "memory_size = 128 # number of features per entry\n",
    "memory_dim = 40 # number of entries in memory\n",
    "learning_rate = 1e-3\n",
    "batch_size = 16\n",
    "\n",
    "n_inputs = width*width+n_classes # input to LSTM cell.\n",
    "n_hnodes = 200 # LSTM cell size\n",
    "n_outputs = n_classes\n",
    "mem_size = memory_size # number of rows in external memory \n",
    "mem_dim = memory_dim # number of columns in external memory\n",
    "n_reads = 4 # number of heads. \n",
    "\n",
    "n_xh = n_inputs+n_hnodes # inputs to LSTM cell (previous state + input image dim)\n",
    "n_rd = n_reads*mem_dim\n",
    "n_hr = n_hnodes+n_rd\n",
    "gamma = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Let's Create all Trainable parameters for network and initialize them.\n",
    "\n",
    "We define all the hyper-parameters and dimensions of all the variables here.\n",
    "\n",
    "This defines three types of weights:\n",
    "* LSTM weights (we use LSTM controller, a feed-forward controller could also be used)\n",
    "* Read and Write key weights\n",
    "* Output fully connected layer weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LSTM gates (4 of them: weights and biases for each)\n",
    "W_gf = torch.Tensor(n_xh, n_hnodes).uniform_(-1., 1.).requires_grad_()\n",
    "b_gf = torch.Tensor(n_hnodes).uniform_(-1., 1.).requires_grad_()\n",
    "W_gi = torch.Tensor(n_xh,n_hnodes).uniform_(-1., 1.).requires_grad_()\n",
    "b_gi = torch.Tensor(n_hnodes).uniform_(-1., 1.).requires_grad_()\n",
    "W_go = torch.Tensor(n_xh,n_hnodes).uniform_(-1., 1.).requires_grad_()\n",
    "b_go = torch.Tensor(n_hnodes).uniform_(-1., 1.).requires_grad_()\n",
    "W_u = torch.Tensor(n_xh,n_hnodes).uniform_(-1., 1.).requires_grad_()\n",
    "b_u = torch.Tensor(n_hnodes).uniform_(-1., 1.).requires_grad_()\n",
    "# Controller Weights\n",
    "W_kr = torch.Tensor(n_hnodes,n_rd).uniform_(-1., 1.).requires_grad_()\n",
    "b_kr = torch.Tensor(n_rd).uniform_(-1., 1.).requires_grad_()\n",
    "W_kw = torch.Tensor(n_hnodes,n_rd).uniform_(-1., 1.).requires_grad_()\n",
    "b_kw = torch.Tensor(n_rd).uniform_(-1., 1.).requires_grad_()\n",
    "W_ga = torch.Tensor(n_hnodes,n_reads).uniform_(-1., 1.).requires_grad_()\n",
    "b_ga = torch.Tensor(n_reads).uniform_(-1., 1.).requires_grad_()\n",
    "# logit weights\n",
    "W_o = torch.Tensor(n_hr,n_outputs).uniform_(-1., 1.).requires_grad_()\n",
    "b_o = torch.Tensor(n_outputs).uniform_(-1., 1.).requires_grad_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Define Model\n",
    "\n",
    "The next cell defines the forward pass of the network. \n",
    "It works as follows:\n",
    "* the net() function receives input X which is sliced along dim 0 which is the time dimension\n",
    "* we sequentially process each time step in run_one_step() function and collect state vectors of each step's output\n",
    "* The predictions are made at each time step with fully connected output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_state0():\n",
    "    # memory variables (not trainable.)\n",
    "    # initialize memory and LSTM states with zero. \n",
    "    return(\n",
    "        torch.FloatTensor(1e-6*np.random.rand(batch_size,mem_size,mem_dim)),\n",
    "        torch.FloatTensor(np.zeros((batch_size,n_hnodes))),\n",
    "        torch.FloatTensor(np.zeros((batch_size,n_hnodes))),\n",
    "        torch.FloatTensor(np.zeros((batch_size,mem_size))),\n",
    "        torch.FloatTensor(np.zeros((batch_size,n_reads,mem_size))),\n",
    "        torch.FloatTensor(np.zeros((batch_size,n_reads,mem_dim))),\n",
    "    )\n",
    "\n",
    "def run_one_step(X_t, state):\n",
    "    # Run one step of the episode.\n",
    "    M_tm1, h_tm1, c_tm1, wu_tm1, wr_tm1, r_tm1 = state\n",
    "    X_t_r = X_t.view(-1,n_inputs)\n",
    "    xh = torch.cat((X_t_r,h_tm1),1)\n",
    "    gf = torch.sigmoid(torch.matmul(xh,W_gf) + b_gf)\n",
    "    gi = torch.sigmoid(torch.matmul(xh,W_gi) + b_gi)\n",
    "    go = torch.sigmoid(torch.matmul(xh,W_go) + b_go)\n",
    "    u_t = torch.tanh(torch.matmul(xh,W_u) + b_u)\n",
    "    c_t = c_tm1*gf + u_t*gi\n",
    "    h_t = c_t*go\n",
    "    kr_t = torch.tanh(torch.matmul(c_t,W_kr) + b_kr).view(batch_size,n_reads,mem_dim)\n",
    "    kw_t = torch.tanh(torch.matmul(c_t,W_kw) + b_kw).view(batch_size,n_reads,mem_dim)\n",
    "    k_norm = torch.norm(kr_t, dim=2, keepdim=True)\n",
    "    m_norm = torch.norm(M_tm1, dim=2, keepdim=True)\n",
    "    inner_prod = torch.matmul(kr_t, M_tm1.permute(0,2,1))\n",
    "    norm_prod = torch.matmul(k_norm, m_norm.permute(0,2,1))\n",
    "    wr_t = F.softmax(inner_prod/norm_prod)\n",
    "    wu_1 = wu_tm1*gamma + torch.sum(wr_t, dim=1)\n",
    "    r_t = torch.matmul(wr_t,M_tm1)\n",
    "    ga = torch.unsqueeze(torch.sigmoid(torch.matmul(h_t,W_ga)+b_ga),2)\n",
    "    _, wlu_inds = torch.topk(-1*wu_1,k=n_reads)\n",
    "    wlu_t = torch.sum(F.one_hot(wlu_inds, mem_size).type(torch.FloatTensor),dim=1,keepdim=True)\n",
    "    ww_t = wr_t*ga + wlu_t*(1-ga)\n",
    "    wu_t = wu_1 + torch.sum(ww_t, dim=1)\n",
    "    M_1 = M_tm1 * (-1*wlu_t).permute(0,2,1)\n",
    "    M_t = M_1 + torch.matmul(ww_t.permute(0,2,1), kw_t)\n",
    "    st8_t = (M_t, h_t, c_t, wu_t, wr_t, r_t)\n",
    "    return st8_t\n",
    "    \n",
    "\n",
    "    \n",
    "def net(X=None, y=None):\n",
    "    # X is of shape (batch_size, None, width, width)\n",
    "    a = np.arange(100*16*405).reshape((100,16,405)).astype(np.float32)\n",
    "    X = torch.from_numpy(a)\n",
    "    \n",
    "    state0 = get_state0()\n",
    "    curr_state = state0\n",
    "    \n",
    "    # Collect output of state vectors in each time step.\n",
    "    M_f = []\n",
    "    h_f = []\n",
    "    c_f = []\n",
    "    wu_f = []\n",
    "    wr_f = []\n",
    "    r_f = []\n",
    "    \n",
    "    for i in range(X.shape[0]):\n",
    "        curr_state = run_one_step(X[i], curr_state)\n",
    "        M_f.append(curr_state[0])\n",
    "        h_f.append(curr_state[1])\n",
    "        c_f.append(curr_state[2])\n",
    "        wu_f.append(curr_state[3])\n",
    "        wr_f.append(curr_state[4])\n",
    "        r_f.append(curr_state[5])\n",
    "        \n",
    "    M_f = torch.stack(M_f)\n",
    "    h_f = torch.stack(h_f)\n",
    "    c_f = torch.stack(c_f)\n",
    "    wu_f = torch.stack(wu_f)\n",
    "    wr_f = torch.stack(wr_f)\n",
    "    r_f = torch.stack(r_f)\n",
    "\n",
    "    hr = torch.cat((h_f, r_f.view(-1,batch_size,n_rd)),2)\n",
    "    o_f = torch.tensordot(hr,W_o,1)+b_o\n",
    "    return (M_f, h_f, c_f, wu_f, wr_f, r_f, o_f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 7: Initialize optimizer and criteria for  training\n",
    "\n",
    "We use Adam optimizer on cross entropy loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam([W_gf, b_gf, W_gi, b_gi, W_go, b_go, W_u, b_u, W_kr,b_kr, W_kw, b_kw,\n",
    "                      W_ga, b_ga, W_o, b_o], lr=learning_rate)\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare graph input\n",
    "\n",
    "Y-labels predicted at each time step are appended to input at next time step for better signal.\n",
    "* During training, we have access to y_labels at time step. Shift the y_labels by one time step and append this to the input to prepare input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_graph_input(X_train, y_train):\n",
    "    X = np.transpose(X_train.reshape(batch_size,-1,width*width),(1,0,2))\n",
    "    y_labels = np.transpose(y_train, (1,0,2))\n",
    "    y_labels_shifted = np.concatenate((np.zeros((1,batch_size,n_classes)), y_labels[:-1,:]),0)\n",
    "    X = np.concatenate((X,y_labels_shifted),-1)\n",
    "    y = np.argmax(y_labels, -1)\n",
    "    return torch.from_numpy(X), torch.from_numpy(y)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 starting..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-30c4c2a2a180>:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  wr_t = F.softmax(inner_prod/norm_prod)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / Batch (1/17) / Loss 156.131744\n",
      "Epoch 1 / Batch (2/17) / Loss 151.174606\n",
      "Epoch 1 / Batch (3/17) / Loss 146.912415\n",
      "Epoch 1 / Batch (4/17) / Loss 147.341263\n",
      "Epoch 1 / Batch (5/17) / Loss 144.785278\n",
      "Epoch 1 / Batch (6/17) / Loss 141.753891\n",
      "Epoch 1 / Batch (7/17) / Loss 139.809174\n",
      "Epoch 1 / Batch (8/17) / Loss 137.314453\n",
      "Epoch 1 / Batch (9/17) / Loss 133.518265\n",
      "Epoch 1 / Batch (10/17) / Loss 131.725983\n",
      "Epoch 1 / Batch (11/17) / Loss 127.548096\n",
      "Epoch 1 / Batch (12/17) / Loss 123.514450\n",
      "Epoch 1 / Batch (13/17) / Loss 119.400345\n",
      "Epoch 1 / Batch (14/17) / Loss 119.333183\n",
      "Epoch 1 / Batch (15/17) / Loss 115.805565\n",
      "Epoch 1 / Batch (16/17) / Loss 114.900040\n",
      "Epoch 1 / Batch (17/17) / Loss 109.105957\n",
      "Epoch 1 complete, 7.910838603973389 seconds elapsed\n",
      "Epoch 2 starting..\n",
      "Epoch 2 complete, 7.675610542297363 seconds elapsed\n",
      "Epoch 3 starting..\n",
      "Epoch 3 complete, 7.75803017616272 seconds elapsed\n",
      "Epoch 4 starting..\n",
      "Epoch 4 complete, 7.873417139053345 seconds elapsed\n",
      "Epoch 5 starting..\n",
      "Epoch 5 complete, 7.832706928253174 seconds elapsed\n",
      "Epoch 6 starting..\n",
      "Epoch 6 complete, 7.5202319622039795 seconds elapsed\n",
      "Epoch 7 starting..\n",
      "Epoch 7 complete, 7.7044007778167725 seconds elapsed\n",
      "Epoch 8 starting..\n",
      "Epoch 8 complete, 7.8522889614105225 seconds elapsed\n",
      "Epoch 9 starting..\n",
      "Epoch 9 complete, 8.495286703109741 seconds elapsed\n",
      "Epoch 10 starting..\n",
      "Epoch 10 complete, 8.453176259994507 seconds elapsed\n",
      "Epoch 11 starting..\n",
      "Epoch 11 / Batch (1/17) / Loss 1.953473\n",
      "Epoch 11 / Batch (2/17) / Loss 1.954194\n",
      "Epoch 11 / Batch (3/17) / Loss 1.901147\n",
      "Epoch 11 / Batch (4/17) / Loss 1.926830\n",
      "Epoch 11 / Batch (5/17) / Loss 1.914495\n",
      "Epoch 11 / Batch (6/17) / Loss 1.908781\n",
      "Epoch 11 / Batch (7/17) / Loss 1.931969\n",
      "Epoch 11 / Batch (8/17) / Loss 1.931852\n",
      "Epoch 11 / Batch (9/17) / Loss 1.921429\n",
      "Epoch 11 / Batch (10/17) / Loss 1.918087\n",
      "Epoch 11 / Batch (11/17) / Loss 1.931054\n",
      "Epoch 11 / Batch (12/17) / Loss 1.955317\n",
      "Epoch 11 / Batch (13/17) / Loss 1.919504\n",
      "Epoch 11 / Batch (14/17) / Loss 1.949403\n",
      "Epoch 11 / Batch (15/17) / Loss 1.919724\n",
      "Epoch 11 / Batch (16/17) / Loss 1.927978\n",
      "Epoch 11 / Batch (17/17) / Loss 1.945055\n",
      "Epoch 11 complete, 8.440156698226929 seconds elapsed\n",
      "Epoch 12 starting..\n",
      "Epoch 12 complete, 7.976708173751831 seconds elapsed\n",
      "Epoch 13 starting..\n",
      "Epoch 13 complete, 7.944493770599365 seconds elapsed\n",
      "Epoch 14 starting..\n",
      "Epoch 14 complete, 7.847809076309204 seconds elapsed\n",
      "Epoch 15 starting..\n",
      "Epoch 15 complete, 7.556804895401001 seconds elapsed\n",
      "Epoch 16 starting..\n",
      "Epoch 16 complete, 7.858496904373169 seconds elapsed\n",
      "Epoch 17 starting..\n",
      "Epoch 17 complete, 7.710097074508667 seconds elapsed\n",
      "Epoch 18 starting..\n",
      "Epoch 18 complete, 7.468553066253662 seconds elapsed\n",
      "Epoch 19 starting..\n",
      "Epoch 19 complete, 7.575081825256348 seconds elapsed\n",
      "Epoch 20 starting..\n",
      "Epoch 20 complete, 7.45098090171814 seconds elapsed\n",
      "Epoch 21 starting..\n",
      "Epoch 21 / Batch (1/17) / Loss 2.132731\n",
      "Epoch 21 / Batch (2/17) / Loss 2.133782\n",
      "Epoch 21 / Batch (3/17) / Loss 2.120271\n",
      "Epoch 21 / Batch (4/17) / Loss 2.089125\n",
      "Epoch 21 / Batch (5/17) / Loss 2.097907\n",
      "Epoch 21 / Batch (6/17) / Loss 2.100850\n",
      "Epoch 21 / Batch (7/17) / Loss 2.099130\n",
      "Epoch 21 / Batch (8/17) / Loss 2.090897\n",
      "Epoch 21 / Batch (9/17) / Loss 2.058316\n",
      "Epoch 21 / Batch (10/17) / Loss 2.078046\n",
      "Epoch 21 / Batch (11/17) / Loss 2.095195\n",
      "Epoch 21 / Batch (12/17) / Loss 2.075256\n",
      "Epoch 21 / Batch (13/17) / Loss 2.064773\n",
      "Epoch 21 / Batch (14/17) / Loss 2.077847\n",
      "Epoch 21 / Batch (15/17) / Loss 2.067600\n",
      "Epoch 21 / Batch (16/17) / Loss 2.077550\n",
      "Epoch 21 / Batch (17/17) / Loss 2.075120\n",
      "Epoch 21 complete, 7.608729362487793 seconds elapsed\n",
      "Epoch 22 starting..\n",
      "Epoch 22 complete, 7.562223196029663 seconds elapsed\n",
      "Epoch 23 starting..\n",
      "Epoch 23 complete, 7.747673988342285 seconds elapsed\n",
      "Epoch 24 starting..\n",
      "Epoch 24 complete, 7.418906927108765 seconds elapsed\n",
      "Epoch 25 starting..\n",
      "Epoch 25 complete, 7.475710391998291 seconds elapsed\n",
      "Epoch 26 starting..\n",
      "Epoch 26 complete, 7.521967887878418 seconds elapsed\n",
      "Epoch 27 starting..\n",
      "Epoch 27 complete, 7.595290184020996 seconds elapsed\n",
      "Epoch 28 starting..\n",
      "Epoch 28 complete, 7.663532495498657 seconds elapsed\n",
      "Epoch 29 starting..\n",
      "Epoch 29 complete, 7.706275701522827 seconds elapsed\n",
      "Epoch 30 starting..\n",
      "Epoch 30 complete, 7.432721853256226 seconds elapsed\n",
      "Epoch 31 starting..\n",
      "Epoch 31 / Batch (1/17) / Loss 1.793914\n",
      "Epoch 31 / Batch (2/17) / Loss 1.784557\n",
      "Epoch 31 / Batch (3/17) / Loss 1.778429\n",
      "Epoch 31 / Batch (4/17) / Loss 1.773425\n",
      "Epoch 31 / Batch (5/17) / Loss 1.768003\n",
      "Epoch 31 / Batch (6/17) / Loss 1.782161\n",
      "Epoch 31 / Batch (7/17) / Loss 1.767317\n",
      "Epoch 31 / Batch (8/17) / Loss 1.769945\n",
      "Epoch 31 / Batch (9/17) / Loss 1.806656\n",
      "Epoch 31 / Batch (10/17) / Loss 1.779488\n",
      "Epoch 31 / Batch (11/17) / Loss 1.804869\n",
      "Epoch 31 / Batch (12/17) / Loss 1.785998\n",
      "Epoch 31 / Batch (13/17) / Loss 1.784626\n",
      "Epoch 31 / Batch (14/17) / Loss 1.801740\n",
      "Epoch 31 / Batch (15/17) / Loss 1.787356\n",
      "Epoch 31 / Batch (16/17) / Loss 1.784623\n",
      "Epoch 31 / Batch (17/17) / Loss 1.782844\n",
      "Epoch 31 complete, 7.671138763427734 seconds elapsed\n",
      "Epoch 32 starting..\n",
      "Epoch 32 complete, 7.832977533340454 seconds elapsed\n",
      "Epoch 33 starting..\n",
      "Epoch 33 complete, 7.794853925704956 seconds elapsed\n",
      "Epoch 34 starting..\n",
      "Epoch 34 complete, 7.548277378082275 seconds elapsed\n",
      "Epoch 35 starting..\n",
      "Epoch 35 complete, 7.504446744918823 seconds elapsed\n",
      "Epoch 36 starting..\n",
      "Epoch 36 complete, 7.5709168910980225 seconds elapsed\n",
      "Epoch 37 starting..\n",
      "Epoch 37 complete, 7.80540132522583 seconds elapsed\n",
      "Epoch 38 starting..\n",
      "Epoch 38 complete, 7.432229995727539 seconds elapsed\n",
      "Epoch 39 starting..\n",
      "Epoch 39 complete, 7.63563346862793 seconds elapsed\n",
      "Epoch 40 starting..\n",
      "Epoch 40 complete, 7.775176048278809 seconds elapsed\n",
      "Epoch 41 starting..\n",
      "Epoch 41 / Batch (1/17) / Loss 1.713431\n",
      "Epoch 41 / Batch (2/17) / Loss 1.718258\n",
      "Epoch 41 / Batch (3/17) / Loss 1.718275\n",
      "Epoch 41 / Batch (4/17) / Loss 1.706926\n",
      "Epoch 41 / Batch (5/17) / Loss 1.732294\n",
      "Epoch 41 / Batch (6/17) / Loss 1.734636\n",
      "Epoch 41 / Batch (7/17) / Loss 1.709437\n",
      "Epoch 41 / Batch (8/17) / Loss 1.722963\n",
      "Epoch 41 / Batch (9/17) / Loss 1.735000\n",
      "Epoch 41 / Batch (10/17) / Loss 1.724092\n",
      "Epoch 41 / Batch (11/17) / Loss 1.728742\n",
      "Epoch 41 / Batch (12/17) / Loss 1.738228\n",
      "Epoch 41 / Batch (13/17) / Loss 1.715606\n",
      "Epoch 41 / Batch (14/17) / Loss 1.729085\n",
      "Epoch 41 / Batch (15/17) / Loss 1.726421\n",
      "Epoch 41 / Batch (16/17) / Loss 1.717593\n",
      "Epoch 41 / Batch (17/17) / Loss 1.733137\n",
      "Epoch 41 complete, 7.630443811416626 seconds elapsed\n",
      "Epoch 42 starting..\n",
      "Epoch 42 complete, 7.619081497192383 seconds elapsed\n",
      "Epoch 43 starting..\n",
      "Epoch 43 complete, 7.865767955780029 seconds elapsed\n",
      "Epoch 44 starting..\n",
      "Epoch 44 complete, 7.739038705825806 seconds elapsed\n",
      "Epoch 45 starting..\n",
      "Epoch 45 complete, 7.626196384429932 seconds elapsed\n",
      "Epoch 46 starting..\n",
      "Epoch 46 complete, 8.146288871765137 seconds elapsed\n",
      "Epoch 47 starting..\n",
      "Epoch 47 complete, 7.797857761383057 seconds elapsed\n",
      "Epoch 48 starting..\n",
      "Epoch 48 complete, 7.709869623184204 seconds elapsed\n",
      "Epoch 49 starting..\n",
      "Epoch 49 complete, 7.8538031578063965 seconds elapsed\n",
      "Epoch 50 starting..\n",
      "Epoch 50 complete, 7.770918130874634 seconds elapsed\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "loss_values =[]\n",
    "for epoch in range(n_epochs):\n",
    "    print('Epoch {} starting..'.format(epoch+1))\n",
    "    epoch_start = time.time()\n",
    "    classes_epoch, imgs_epoch = ut.shuffle_xy(classes_train,imgs_train) \n",
    "\n",
    "    n_batches = len(classes_epoch)//(n_classes*batch_size)\n",
    "    for batch in range(n_batches):\n",
    "        classes_batch = classes_epoch[batch*n_classes*batch_size:(batch+1)*n_classes*batch_size]\n",
    "        imgs_batch = imgs_epoch[batch*n_classes*batch_size:(batch+1)*n_classes*batch_size]\n",
    "\n",
    "        Xl_batch, yl_batch = [], []\n",
    "        for episode in range(batch_size):\n",
    "            imgs_ep = imgs_batch[episode*n_classes:(episode+1)*n_classes]\n",
    "            Xl_ep, yl_ep = [], []\n",
    "            for ind, cat in enumerate(imgs_ep):\n",
    "                for arr in cat:\n",
    "                    Xl_ep.append(arr)\n",
    "                    yl_ep.append(ut.one_hot(ind,n_classes))\n",
    "            Xl_shuff, yl_shuff = ut.shuffle_xy(Xl_ep,yl_ep)\n",
    "            X_arr, y_arr = np.asarray(Xl_shuff), np.asarray(yl_shuff)\n",
    "            Xl_batch.append(X_arr)\n",
    "            yl_batch.append(y_arr)\n",
    "        X_train, y_train = np.asarray(Xl_batch), np.asarray(yl_batch)\n",
    "        X_mann, y_mann = get_graph_input(X_train, y_train)\n",
    "        optimizer.zero_grad()\n",
    "        M, h, c, wu, wr, r, o = net(X_mann)\n",
    "        outs = o.view(100*16, 5)\n",
    "        gt = y_mann.view(100*16)\n",
    "        loss = criterion(outs, gt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_d = loss.item()\n",
    "        _, predicted = torch.max(outs.data, 1)\n",
    "        correct = (predicted == gt).sum().item()\n",
    "        loss_values.append(loss_d)\n",
    "        if(epoch%10==0):\n",
    "            print('Epoch %d'%(epoch+1),'/ Batch (%d/%d)'%(batch+1,n_batches),'/ Loss %f'%(loss_d))\n",
    "\n",
    "    epoch_end = time.time()\n",
    "    time_elapsed = epoch_end-epoch_start\n",
    "    print('Epoch {} complete,'.format(epoch+1),time_elapsed,'seconds elapsed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_loss(train,name1=\"train_loss\"):\n",
    "    plt.plot(train, label=name1)\n",
    "    plt.legend()\n",
    "\n",
    "plot_loss(loss_values)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
